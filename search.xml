<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>卷积神经网络图像识别</title>
      <link href="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="手写卷积神经网络图像识别——对比pytorch"><a href="#手写卷积神经网络图像识别——对比pytorch" class="headerlink" title="手写卷积神经网络图像识别——对比pytorch"></a>手写卷积神经网络图像识别——对比pytorch</h1><script type="math/tex; mode=display">\textsf{@Justzl}</script><h1 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h1><ul><li>理解卷积神经网络的基本结构、代码实现及训练过程</li><li>应用dropout和多种normalization方法，理解它们对模型泛化能力的影响</li><li>理解如何通过交叉验证，为神经网络找到最好的hyperparameters</li><li>在训练网络的过程中，可根据需要自由尝试其它提升性能的方法，例如通过增加模型层数、使用不同的正则化方法、使用模型集成</li></ul><h1 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h1><p>详见代码文件中<strong>read.md</strong></p><h1 id="数据集说明"><a href="#数据集说明" class="headerlink" title="数据集说明"></a>数据集说明</h1><p>该数据集共有60000张彩色图像，这些图像是32*32，分为10个类，每类6000张图。这里面有50000张用于训练，构成了5个训练批，每一批10000张图；另外10000用于测试，单独构成一批。测试批的数据里，取自10类中的每一类，每一类随机取1000张。抽剩下的就随机排列组成了训练批。注意一个训练批中的各类图像并不一定数量相同，总的来看训练批，每一类都有5000张图。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled.png" alt="Untitled0"></p><p>下载解压后数据格式如下，以字典形式存储。每个batch字典包含<strong>data</strong>和<strong>labels</strong>，其中<strong>data</strong>是10000<em>3072的数组，每1024列为图片R/G/B通道，其数据组织为**10000张3通道的32</em>32像素图片batchmeta<strong>中包含的是labels与实际对应的类名，使用右图两个方法来</strong>解包<strong>数据和进行</strong>独热编码**</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%201.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%202.png" alt="Untitled"></p><p>下图是训练batch1中的数据和batchesmeta中的字典，右图为训练集和测试集中的某一张图片。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%203.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%204.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%205.png" alt="Untitled"></p><h1 id="基于numpy手写卷积神经网络"><a href="#基于numpy手写卷积神经网络" class="headerlink" title="基于numpy手写卷积神经网络"></a>基于numpy手写卷积神经网络</h1><h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h2><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%206.png" alt="Untitled"></p><p>上图为导入全部的训练数据（测试集同理），将所有数据导入后，得到4个数组，规模如下，3代表rgb通道值。</p><div class="table-container"><table><thead><tr><th>数据集</th><th>规模</th></tr></thead><tbody><tr><td>train_data</td><td>50000<em>3</em>32*32</td></tr><tr><td>train_labels</td><td>50000<em>3</em>32*32</td></tr><tr><td>test_data</td><td>10000<em>3</em>32*32</td></tr><tr><td>test_labels</td><td>10000<em>3</em>32*32</td></tr></tbody></table></div><h2 id="定义卷积神经网络"><a href="#定义卷积神经网络" class="headerlink" title="定义卷积神经网络"></a>定义卷积神经网络</h2><p>考虑较为简单的卷积神经网络，结构入下图所示，只考虑一层卷积，一层池化的情况</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%207.png" alt="Untitled"></p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>考虑通用卷积操作，假设输入为<strong>3<em>32</em>32</strong>，卷积核为<strong>3<em>2</em>2*2</strong>，则输出为<strong>2<em>16</em>16</strong></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%208.png" alt="Untitled"></p><p>首先想到的是对于<strong>每一个样本每一个通道的每一个行和列取与卷积核大小相同的数据做运算</strong>，但是这样效率实在是太低，计算很慢。（如下代码）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convolute</span>(<span class="params">X,W,Sw,Sh</span>):</span><br><span class="line">    Xw=X.shape[<span class="number">2</span>]</span><br><span class="line">    Xh=X.shape[<span class="number">3</span>]</span><br><span class="line">    Ww=W.shape[<span class="number">2</span>]</span><br><span class="line">    Wh=W.shape[<span class="number">3</span>]</span><br><span class="line">    Yw=<span class="built_in">int</span>((Xw-Ww)/Sw )+<span class="number">1</span></span><br><span class="line">    Yh=<span class="built_in">int</span>((Xh-Wh)/Sh )+<span class="number">1</span></span><br><span class="line">    Y=np.zeros((X.shape[<span class="number">0</span>], W.shape[<span class="number">1</span>],Yw ,Yh))</span><br><span class="line">    <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span> (X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span> (W.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[rank][y]=(signal.convolve2d(X[rank][<span class="number">0</span>],W[<span class="number">0</span>][y],mode=<span class="string">&#x27;valid&#x27;</span>, boundary=<span class="string">&#x27;wrap&#x27;</span>)[::Sh, ::Sw]</span><br><span class="line">                                      +signal.convolve2d(X[rank][<span class="number">1</span>],W[<span class="number">1</span>][y],mode=<span class="string">&#x27;valid&#x27;</span>, boundary=<span class="string">&#x27;wrap&#x27;</span>)[::Sh, ::Sw]</span><br><span class="line">                                      +signal.convolve2d(X[rank][<span class="number">2</span>],W[<span class="number">2</span>][y],mode=<span class="string">&#x27;valid&#x27;</span>, boundary=<span class="string">&#x27;wrap&#x27;</span>)[::Sh, ::Sw])</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>故采用<strong>矩阵相乘</strong>的方式来计算卷积。先考虑最简单的一次卷积运算，如下图，由图可知<strong>输入数据与卷积核卷积等价于将输入数据展平与卷积核展平后的转置矩乘</strong>。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%209.png" alt="Untitled"></p><p>故要将卷积核与一整个输入进行卷积，则先将与卷积核同维度的窗口在输入数据上滑动，每次<strong>将滑动得到的矩阵展平并存储下来</strong>，得到一个<strong>Kh*Km的矩阵K。</strong>这里的K的形状为<strong>[16*16,4]。</strong>则对于每个32*32的输入图像，都可通过滑动求得其K矩阵</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%25E5%25BE%25AE%25E4%25BF%25A1%25E5%259B%25BE%25E7%2589%2587_20231029153736.jpg" alt="微信图片_20231029153736.jpg"></p><p>现在我们要将3通道的输入数据和3通道的卷积核矩乘，则对于每个通道，先滑动得到K矩阵，再将K矩阵和卷积核矩乘，得到16*16的向量，将其展开为二维矩阵即为输出矩阵。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2010.png" alt="Untitled"></p><p>再对第二层的三个卷积核执行同样的操作，即可得到2<em>16</em>16的输出矩阵</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2011.png" alt="Untitled"></p><p>用代码实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convolute</span>(<span class="params">X,W,Sh,Sw,bias</span>):</span><br><span class="line">    Xh=X.shape[<span class="number">2</span>]</span><br><span class="line">    Xw=X.shape[<span class="number">3</span>]</span><br><span class="line">    Wh=W.shape[<span class="number">2</span>]</span><br><span class="line">    Ww=W.shape[<span class="number">3</span>]</span><br><span class="line">    Yh=<span class="built_in">int</span>((Xh-Wh)/Sh )+<span class="number">1</span></span><br><span class="line">    Yw=<span class="built_in">int</span>((Xw-Ww)/Sw )+<span class="number">1</span></span><br><span class="line">    <span class="comment">#K为将卷积核在输入数据上滑动的矩阵依次展开并展平</span></span><br><span class="line">    K=np.zeros((X.shape[<span class="number">0</span>],X.shape[<span class="number">1</span>],Yw*Yh,Ww*Wh))</span><br><span class="line">    Y=np.zeros((X.shape[<span class="number">0</span>], W.shape[<span class="number">1</span>],Yh*Yw,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]): <span class="comment">#对每一个样本</span></span><br><span class="line">        <span class="keyword">for</span> rgb <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">            temp =<span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Yh):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Yw):</span><br><span class="line">                    sample=X[rank,rgb,i*Sh:i*Sh+Wh,j*Sw:j*Sw+Ww]</span><br><span class="line">                    K[rank,rgb,temp]=sample.flatten()</span><br><span class="line">                    temp+=<span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#每个通道的k计算后与卷积核相乘</span></span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(W.shape[<span class="number">1</span>]):</span><br><span class="line">                Y[rank,m]=Y[rank,m]+np.dot(K[rank,rgb],W[rgb,m].reshape((Wh*Ww,<span class="number">1</span>)))+bias[m]</span><br><span class="line">    </span><br><span class="line">    Yout=Y.reshape((Y.shape[<span class="number">0</span>],Y.shape[<span class="number">1</span>],Yh,Yw))</span><br><span class="line">    <span class="keyword">return</span> Yout,K</span><br></pre></td></tr></table></figure><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>采用最大池化的方式计算，每次只取滑动窗口中最大的值为保留值，并记录下该值<strong>位于窗口中的具体位置到Mcin</strong>，其余值舍弃。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_pooling</span>(<span class="params">X,pool_size=(<span class="params"><span class="number">2</span>,<span class="number">2</span></span>)</span>):</span><br><span class="line">    ph,pw=pool_size</span><br><span class="line">    Mcin=np.zeros_like(X)</span><br><span class="line">    X_copy=X</span><br><span class="line"><span class="comment">#若数据规模无法被滑动窗口整除，则在矩阵周围补上与滑动窗口大小相同的几层0</span></span><br><span class="line">    <span class="keyword">if</span> X_copy.shape[<span class="number">2</span>] % ph !=<span class="number">0</span>:</span><br><span class="line">        plusw=np.zeros((X_copy.shape[<span class="number">0</span>],X_copy.shape[<span class="number">1</span>],ph-X_copy.shape[<span class="number">2</span>] % ph,X_copy.shape[<span class="number">3</span>]))</span><br><span class="line">        X_copy=np.concatenate((X_copy,plusw), axis=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span>  X.shape[<span class="number">3</span>] % pw !=<span class="number">0</span>:</span><br><span class="line">        plush=np.zeros((X_copy.shape[<span class="number">0</span>],X_copy.shape[<span class="number">1</span>],X_copy.shape[<span class="number">2</span>],pw-X_copy.shape[<span class="number">3</span>] % pw))</span><br><span class="line">        X_copy=np.concatenate((X_copy,plush), axis=<span class="number">3</span>)</span><br><span class="line">    <span class="comment">#计算输出形状时向下取整取，确保利用上了所有数据并且不溢出</span></span><br><span class="line">outh=X_copy.shape[<span class="number">2</span>]//ph</span><br><span class="line">    outw=X_copy.shape[<span class="number">3</span>]//pw</span><br><span class="line">    Out=np.zeros((X.shape[<span class="number">0</span>],X.shape[<span class="number">1</span>],outh,outw))</span><br><span class="line">    <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span> (X_copy.shape[<span class="number">0</span>]): <span class="comment">#对每一个样本</span></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span> (X_copy.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (outh):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(outw):</span><br><span class="line">                    window = X_copy[rank][y][i * ph:(i + <span class="number">1</span>) * ph, j * pw:(j + <span class="number">1</span>) * pw]</span><br><span class="line">                    max_index = np.unravel_index(np.argmax(window), window.shape)</span><br><span class="line">                    <span class="comment">#将最大值记录，并保留位置信息</span></span><br><span class="line">                    Out[rank][y][i][j]=np.<span class="built_in">max</span>(window)</span><br><span class="line">                    Mcin[rank][y][i * ph+max_index[<span class="number">0</span>]][j * pw+max_index[<span class="number">1</span>]]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> Out,Mcin</span><br></pre></td></tr></table></figure><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>调用之前自己写的手写数字识别神经网络框架（更名为<strong>neuralnet.py</strong>），该框架为3层全连接神经网络，神经元数量，学习率，正则化，损失函数等等均可自定义更改。详情见</p><p><a href="https://www.notion.so/MINIST-edcb905e268945629ae40d55091e401d?pvs=21">手写神经网络手写数字识别—MINIST</a></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2012.png" alt="Untitled"></p><h2 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h2><p>训练过程如下</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2013.png" alt="Untitled"></p><p>构建网络后先进行一次前向传播以确立全连接网络形状，前向传播的过程省略，下面来看<strong>网络反向传播更新参数</strong>的过程。</p><h3 id="全连接层反向传播"><a href="#全连接层反向传播" class="headerlink" title="全连接层反向传播"></a>全连接层反向传播</h3><p>计算预测值和真实值的误差项，并逐层回传更新参数W和B，在误差传递到输入层时回传输出delta_back为池化层的误差，过程省略，可更改参数实现更多优化方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">delta_back,self.loss=self.network.backforward(self.nninput, </span><br><span class="line">     output_label[batch_size*batch:(batch+<span class="number">1</span>)*batch_size],</span><br><span class="line">     learning_rate=<span class="number">0.0052564</span>,</span><br><span class="line">     lossfunction=neuralnet.cross_entropy,</span><br><span class="line">     reg_strength=<span class="number">0</span>,</span><br><span class="line">     dfx=Leakyrelu_derivative,</span><br><span class="line">     adam=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="最大池化层反向传播"><a href="#最大池化层反向传播" class="headerlink" title="最大池化层反向传播"></a>最大池化层反向传播</h3><p>前文提到最大池化将每一窗口的最大值保留，其余丢弃。则<strong>池化层中输入数据的误差项为已保留数据的误差项</strong>，其余未保留数据的误差为0。利用前文存下的Mcin矩阵，则可以将误差项传给对应位置的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_pooling_derivate</span>(<span class="params">Mcin,delta,pool_size=(<span class="params"><span class="number">2</span>,<span class="number">2</span></span>)</span>):</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(Mcin.shape[<span class="number">0</span>]): <span class="comment">#对每一个样本</span></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(Mcin.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(delta.shape[<span class="number">2</span>]):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(delta.shape[<span class="number">3</span>]):</span><br><span class="line">                    Mcin[r][y][i*pool_size[<span class="number">0</span>]:(i+<span class="number">1</span>)*pool_size[<span class="number">0</span>]][j*pool_size[<span class="number">1</span>]:(j+<span class="number">1</span>)*pool_size[<span class="number">1</span>]]=Mcin[r][y][i*pool_size[<span class="number">0</span>]:(i+<span class="number">1</span>)*pool_size[<span class="number">0</span>]][j*pool_size[<span class="number">1</span>]:(j+<span class="number">1</span>)*pool_size[<span class="number">1</span>]]*delta[r][y][i][j]</span><br><span class="line">    <span class="keyword">return</span> Mcin</span><br></pre></td></tr></table></figure><h3 id="卷积层反向传播"><a href="#卷积层反向传播" class="headerlink" title="卷积层反向传播"></a>卷积层反向传播</h3><p>现在已知从池化层回传的误差，在将其经过激活函数的导数后回传到卷积层。卷积核上点A显然对卷积结果每一个点都有影响。它对卷积结果的影响等于将整个原图左上3×3的部分乘上点A的值，因此delta误差反向传播回时，<strong>点A的导数等于卷积结果的delta误差与原图左上3×3红色部分逐点相乘后求和</strong>。因此二维卷积核的导数等于原图对应通道与卷积结果对应通道的delta误差直接进行卷积<strong>。</strong></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2014.png" alt="Untitled"></p><p>我们的B是一个列向量，它给卷积结果的每一个通道都加上同一个标量。因此，在反向传播时，它的导数等于卷积结果的delta误差在每一个通道上将所有delta误差进行求和的结果。</p><script type="math/tex; mode=display">\frac{\partial C}{\partial w^l}  = \frac{\partial C}{\partial z^l}\frac{\partial z^l}{\partial w^l}=\delta^l*a^{l-1}\\\frac{\partial C}{\partial b^l}  = \frac{\partial C}{\partial z^l}\frac{\partial z^l}{\partial b^l}=\sum _{x}\sum _{y}\delta^l</script><p>这时我们之前存下的K矩阵就派上用场了，想求出卷积核的导数，只需将K矩阵和误差项直接矩乘即可得到卷积核的导数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">W_derivate</span>(<span class="params">W,delta,K</span>):</span><br><span class="line">    Y=np.zeros((W.shape[<span class="number">0</span>],W.shape[<span class="number">1</span>],W.shape[<span class="number">2</span>]*W.shape[<span class="number">3</span>],<span class="number">1</span>))</span><br><span class="line">    delta=delta.reshape((delta.shape[<span class="number">0</span>],delta.shape[<span class="number">1</span>],delta.shape[<span class="number">2</span>]*delta.shape[<span class="number">3</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span>(delta.shape[<span class="number">0</span>]):</span><br><span class="line">                Y[i,j]=Y[i,j]+np.dot(delta[rank,j].T,K[rank,i]).T</span><br><span class="line">    Yout=Y/delta.shape[<span class="number">0</span>]</span><br><span class="line">    Yout=Yout.reshape((W.shape[<span class="number">0</span>],W.shape[<span class="number">1</span>],W.shape[<span class="number">2</span>],W.shape[<span class="number">3</span>]))</span><br><span class="line">    <span class="keyword">return</span> Yout</span><br></pre></td></tr></table></figure><h3 id="adam优化算法"><a href="#adam优化算法" class="headerlink" title="adam优化算法"></a>adam优化算法</h3><p>卷积部分反向传播如下，对其使用adam优化算法，原理同全连接神经网络，这里不再赘述。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2015.png" alt="Untitled"></p><h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><div class="table-container"><table><thead><tr><th>EPOCH</th><th>卷积层学习率</th><th>全连接学习率</th><th>优化算法</th><th>最低loss值</th><th>Test-ACC</th></tr></thead><tbody><tr><td>100</td><td>0.0085678</td><td>0.0052564</td><td>无</td><td>1.4319</td><td>47.74%</td></tr><tr><td>20</td><td>0.0085678</td><td>0.0052564</td><td>Adam</td><td>1.0998</td><td>63.6%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2016.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2017.png" alt="Untitled"></p><h1 id="基于pytorch搭建卷积神经网络"><a href="#基于pytorch搭建卷积神经网络" class="headerlink" title="基于pytorch搭建卷积神经网络"></a>基于pytorch搭建卷积神经网络</h1><h2 id="数据增强并导入"><a href="#数据增强并导入" class="headerlink" title="数据增强并导入"></a>数据增强并导入</h2><p><strong>调用torchvision的transforms库增强数据，利用datasets导入cifar数据集，使用dataloader进行数据分批。</strong></p><ul><li><p>数据增强</p><p>  数据增强可以帮助改善模型的泛化性能，减少过拟合。以下是使用的一些数据增强方法以及代码</p><p>  | 方法 | 作用 |<br>  | —- | —- |<br>  | RandomHorizontalFlip() | 进行随机水平翻转（镜像）以增加数据的多样性，帮助神经网络更好地学习不同角度的物体。 |<br>  | RandomGrayscale() | 以一定的概率随机将图像转换为灰度图像，以使模型对灰度图像的变化具有鲁棒性。 |<br>  | ColorJitter<br>  (brightness=0.2,contrast=0.2,saturation=0.2, hue=0.1) | 随机颜色变换 |<br>  | ToTenso() | 将图像数据转换为PyTorch张量（tensor），以便神经网络可以处理。 |<br>  | Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) | 对图像进行标准化，将像素值从范围[0, 1] 缩放到范围[-1, 1] |</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">transform_train = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(), </span><br><span class="line">    transforms.RandomGrayscale(),   </span><br><span class="line">transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0.1</span>),      </span><br><span class="line">transforms.ToTensor(),         </span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])  </span><br><span class="line">transform_test = transforms.Compose([     </span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br></pre></td></tr></table></figure></li><li><p>导入数据<br>使用<strong>DataLoader</strong>进行数据分批，<strong>dataset</strong>代表传入的数据集，<strong>batch_size</strong>表示每个batch有多少个样本，<strong>shuffle</strong>表示在每个epoch开始的时候，对数据进行<strong>重新排序。</strong></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">125</span></span><br><span class="line"><span class="comment">#将数据加载进来，本地已经下载好，    </span></span><br><span class="line">train_data = datasets.CIFAR10(root=<span class="string">r&#x27;.\data\cifar-10-python&#x27;</span>,train=<span class="literal">True</span>,transform=transform_train,download=<span class="literal">False</span>)</span><br><span class="line">test_data =datasets.CIFAR10(root=<span class="string">r&#x27;.\data\cifar-10-python&#x27;</span>,train=<span class="literal">False</span>,transform=transform_test,download=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#数据分批</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">train_loader = DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li>数据分批之前：<br><strong>torch.Size</strong>([3, 32, 32])：<br><strong>Tensor</strong>[[32<em>32][32</em>32][32*32]],<br>每一个元素都是归一化之后的RGB的值；</li><li><p>数据分批之前：<br><strong>train_data</strong>([50000[3<em>[32</em>32]]])</p></li><li><p>数据分批之后：<br><strong>torch.Size</strong>([64, 3, 32, 32])</p></li><li>数据分批之后：<br><strong>train_loader</strong>([50000/64<em>[64</em>[3<em>[32</em>32]]]])</li></ul><h2 id="设计网络结构"><a href="#设计网络结构" class="headerlink" title="设计网络结构"></a>设计网络结构</h2><h3 id="简单卷积神经网络"><a href="#简单卷积神经网络" class="headerlink" title="简单卷积神经网络"></a>简单卷积神经网络</h3><p>考虑较简单的卷积神经网络，即只有一层卷积层和一层池化层，以下是ConvNet1的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搭建卷积神经网络模型</span></span><br><span class="line"><span class="comment"># 1个卷积层,3个全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvNet1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvNet1, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">16</span>,kernel_size=<span class="number">2</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.1</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">            )</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">16</span> * <span class="number">16</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">128</span>, num_classes)</span><br><span class="line">        <span class="comment"># 定义前向传播顺序</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        out = self.fc3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_class_name</span>(<span class="params">cls</span>):</span><br><span class="line">        <span class="keyword">return</span> cls.__name__</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>网络结构</th><th>EPOCH</th><th>BN</th><th>优化算法</th><th>Test-ACC</th></tr></thead><tbody><tr><td>ConNet1</td><td>30</td><td>N</td><td>Adam</td><td>63.39%</td></tr><tr><td>ConNet1</td><td>30</td><td>Y</td><td>Adam</td><td>63.14%</td></tr><tr><td>ConNet1</td><td>30</td><td>N</td><td>SGD</td><td>63.33%</td></tr><tr><td>ConNet1</td><td>30</td><td>Y</td><td>SGD</td><td>64.78%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2018.png" alt="Untitled"></p><p>左图为分别使用Adam训练的有无添加BN的训练集和测试集loss，右图为是否启用BN的Acc</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2019.png" alt="Untitled"></p><p>同ConvNet1我们可以实现一个三层卷积层的神经网络ConvNet3，其训练结果如下</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2020.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2021.png" alt="Untitled"></p><p>为其添加dropout正则化，并在全连接层之间使用激活函数激活</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2022.png" alt="Untitled"></p><div class="table-container"><table><thead><tr><th>网络结构</th><th>EPOCH</th><th>dropout</th><th>优化算法</th><th>Test-ACC</th></tr></thead><tbody><tr><td>ConNet3</td><td>30</td><td>0.5</td><td>SGD</td><td>64.47%</td></tr><tr><td>ConNet3</td><td>30</td><td>0.1</td><td>SGD</td><td>72.29%</td></tr><tr><td>ConNet3</td><td>30</td><td>0.03</td><td>SGD</td><td>74.13%</td></tr><tr><td>ConNet3</td><td>30</td><td>0.001</td><td>SGD</td><td>70.55%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2023.png" alt="Untitled"></p><p>更多数据详情启动TensorBoard查看</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2024.png" alt="Untitled"></p><h3 id="较深卷积神经网络"><a href="#较深卷积神经网络" class="headerlink" title="较深卷积神经网络"></a>较深卷积神经网络</h3><p>可以看出，前面的两个网络都不能很好的学习到数据里的内容，不能很好的学会数据中的特征。我们加大网络的卷积层数，并加宽数据通道数，构建一个6层卷积神经网络ConvNet6（代码略）</p><p>我们采用5折交叉验证的方式进行训练，每个折训练30次，下图是模型在训练集和验证集上的loss曲线.数据详情见<strong>Tensorboard</strong></p><div class="table-container"><table><thead><tr><th>网络结构</th><th>EPOCH</th><th>dropout</th><th>优化算法</th><th>Test-ACC</th></tr></thead><tbody><tr><td>ConNet6</td><td>30</td><td>0</td><td>Adam</td><td>80.55%</td></tr><tr><td>ConNet6</td><td>30</td><td>0.35</td><td>Adam</td><td>81.3%</td></tr><tr><td>ConNet6</td><td>30</td><td>0.5</td><td>Adam</td><td>81.86%</td></tr><tr><td>ConNet6</td><td>30</td><td>0.7</td><td>Adam</td><td>81.14%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2025.png" alt="Untitled"></p><p>还可以看到每一次卷积后图像的变化，以Convnet6为例</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2026.png" alt="Untitled"></p><h1 id="基于预训练模型搭建卷积神经网络"><a href="#基于预训练模型搭建卷积神经网络" class="headerlink" title="基于预训练模型搭建卷积神经网络"></a>基于预训练模型搭建卷积神经网络</h1><h2 id="测试不同模型"><a href="#测试不同模型" class="headerlink" title="测试不同模型"></a>测试不同模型</h2><div class="table-container"><table><thead><tr><th>模型</th><th>优化算法</th><th>准确率</th></tr></thead><tbody><tr><td>VGG16</td><td>Adam</td><td>85.34%</td></tr><tr><td>ResNet50</td><td>Adam</td><td>83.2%</td></tr><tr><td>DenseNet121</td><td>Adam</td><td>86.02%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2027.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2028.png" alt="Untitled"></p><p><strong>利用tensorboard的addgraph功能可以清晰的看到各个模型内部的结构以及数据张量的流动</strong></p><p><strong>（高清图见tensorboard内ConvolutionNN\logs\model_vision）</strong></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2029.png" alt="Untitled"></p><h2 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h2><p>在预测时，对所有模型的输出，采用投票的形式决定最终的预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnsembleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, models</span>):</span><br><span class="line">        <span class="built_in">super</span>(EnsembleModel, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> models <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.models = nn.ModuleList(models)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.models = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">if</span> self.models <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> inputs  <span class="comment"># Pass through the input if there are no models in the ensemble</span></span><br><span class="line">        outputs = [model(inputs) <span class="keyword">for</span> model <span class="keyword">in</span> self.models]</span><br><span class="line">        <span class="keyword">return</span> torch.stack(outputs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入模型</span></span><br><span class="line">model_vgg16 = torch.load((<span class="string">r&#x27;.\CNNmodel\VGG16Adambestfold.pt&#x27;</span>))</span><br><span class="line">model_resnet50 = torch.load((<span class="string">r&#x27;.\CNNmodel\ResNet50Adambestfold.pt&#x27;</span>))</span><br><span class="line">model_densenet101 = torch.load((<span class="string">r&#x27;.\CNNmodel\DenseNet121Adambestfold.pt&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型集成</span></span><br><span class="line">models = [model_vgg16, model_resnet50, model_densenet101]</span><br><span class="line">ensemble_model = EnsembleModel(models)</span><br><span class="line">predictions = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images = images.to(device)  </span><br><span class="line">        outputs = ensemble_model(images)</span><br><span class="line">        <span class="comment"># 统计每个模型的预测结果# 将输入数据移动到GPU</span></span><br><span class="line">        ensemble_predictions = outputs.<span class="built_in">max</span>(dim=<span class="number">2</span>)[<span class="number">0</span>].mode(dim=<span class="number">0</span>).values</span><br><span class="line">        predictions.extend(ensemble_predictions.cpu().numpy())</span><br><span class="line">         <span class="comment"># 计算准确率</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predictions == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy on the test set: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算最终的投票结果</span></span><br><span class="line">final_predictions = [torch.mode(torch.tensor(prediction))[<span class="number">0</span>].item() <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions]</span><br></pre></td></tr></table></figure><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2030.png" alt="Untitled"></p><h1 id="附录：TensorBoard可视化使用（vscode）"><a href="#附录：TensorBoard可视化使用（vscode）" class="headerlink" title="附录：TensorBoard可视化使用（vscode）"></a>附录：TensorBoard可视化使用（vscode）</h1><p>tensorboard是一个非常强大的工具、不仅仅可以帮助我们可视化神经网络训练过程中的各种参数，而且可以帮助我们更好的调整网络模型、网络参数。此处仅介绍在代码中的基本使用。（先在vscode扩展中下载tensroboard。）</p><p>在代码中启动tensorboard，logdir为你想存放日志文件的位置，即生成数据的位置</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2031.png" alt="Untitled"></p><p>使用addscaler函数添加loss值，第一个参数为图名称，第二个为loos值，第三个为迭代次数</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2032.png" alt="Untitled"></p><p>使用addtext函数添加text值，这里用于存放每一次预测准确率</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2033.png" alt="Untitled"></p><p>使用addgraph函数添加模型计算图，testinput为样例输入</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2034.png" alt="Untitled"></p><p>Tensorboard类的参数列表如下</p><ul><li><p><strong>log_dir:</strong></p></li><li><p>用来保存被 TensorBoard 分析的日志文件的文件名。</p></li><li><p><strong>histogram_freq</strong></p></li><li><p>对于模型中各个层计算激活值和模型权重直方图的频率（训练轮数中）。 如果设置成 0 ，直方图不会被计算。对于直方图可视化的验证数据（或分离数据）一定要明确的指出。</p></li><li><p><strong>write_graph</strong>:</p></li><li><p>是否在 TensorBoard 中可视化图像。 如果 write_graph 被设置为 True。</p></li><li><p><strong>write_grads</strong>:</p></li><li><p>是否在 TensorBoard 中可视化梯度值直方图。 histogram_freq 必须要大于 0 。</p></li><li><p><strong>batch_size</strong>:</p></li><li><p>用以直方图计算的传入神经元网络输入批的大小。</p></li><li><p><strong>write_images</strong>:</p></li><li><p>是否在 TensorBoard 中将模型权重以图片可视化，如果设置为True，日志文件会变得非常大。</p></li><li><p><strong>embeddings_freq</strong>:</p></li><li><p>被选中的嵌入层会被保存的频率（在训练轮中）。</p></li><li><p><strong>embeddings_layer_names</strong>:</p></li><li><p>一个列表，会被监测层的名字。 如果是 None 或空列表，那么所有的嵌入层都会被监测。</p></li><li><p><strong>embeddings_metadata</strong>:</p></li><li><p>一个字典，对应层的名字到保存有这个嵌入层元数据文件的名字。 查看 详情 关于元数据的数据格式。 以防同样的元数据被用于所用的嵌入层，字符串可以被传入。</p></li><li><p><strong>embeddings_data</strong>:</p></li><li><p>要嵌入在 embeddings_layer_names 指定的层的数据。 Numpy 数组（如果模型有单个输入）或 Numpy 数组列表（如果模型有多个输入）。 Learn ore about embeddings。</p></li><li><p><strong>update_freq</strong>:</p></li><li><p>‘batch’ 或 ‘epoch’ 或 整数。当使用 ‘batch’ 时，在每个 batch 之后将损失和评估值写入到 TensorBoard 中。同样的情况应用到 ‘epoch’ 中。如果使用整数，例如 10000，这个回调会在每 10000 个样本之后将损失和评估值写入到 TensorBoard 中。注意，频繁地写入到 TensorBoard 会减缓你的训练。</p></li></ul><p><a href="https://www.notion.so/pytorch-7eb2d2014d4d4e7f8b291ffe7327fb31?pvs=21">手写卷积神经网络与pytorch神经网络说明</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>HMM</title>
      <link href="/2023/05/17/HMM/"/>
      <url>/2023/05/17/HMM/</url>
      
        <content type="html"><![CDATA[<h1 id="HMM学习"><a href="#HMM学习" class="headerlink" title="HMM学习"></a>HMM学习</h1><p>HMM是隐马尔可夫模型（Hidden Markov Model）的缩写，是一种用于表示时间序列数据的概率模型，常用于语音识别、自然语言处理、生物信息学等领域。</p><p>使用HMM模型时我们的问题一般有这两个特征：</p><ol><li>我们的问题是<strong>基于序列</strong>的，比如时间序列，或者状态序列。</li><li>我们的问题中有两类数据，<strong>一类序列数据是可以观测到的</strong>，即观测序列；而<strong>另一类数据是不能观察</strong>到的，即隐藏状态序列，简称状态序列。</li></ol><h1 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h1><p>假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里球的数量分别是：</p><div class="table-container"><table><thead><tr><th>盒子</th><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>红球数</td><td>5</td><td>4</td><td>7</td></tr><tr><td>白球数</td><td>5</td><td>6</td><td>3</td></tr></tbody></table></div><p>再开始抽球之前，对每个盒子都存在初始概率，即第一次抽到1/2/3号盒子的概率，假设是0.2/0.4/0.4,此为<strong>初始状态矩阵</strong></p><script type="math/tex; mode=display">\pi=(0.2,0.4,0.4)^{T}</script><p>抽球的规则必须遵守如下：</p><ul><li>我们无法看到球从哪个盒子里被取出，我们<strong>只能看到取出球的颜色。</strong>假设取出的是红球，其从1号盒子出来的概率是0.5，从2号盒子里出来的概率是0.4，从3号盒子里出来的概率是0.7，我们就得到了红球的观察状态概率矩阵，同理假设白球从1号盒子里取出的概率是0.5 ，从2号盒子里出来的概率是0.6，从3号盒子里出来的概率是0.3，就得到了<strong>发射概率矩阵</strong></li></ul><pre><code>|  | 红球 | 白球 || --- | --- | --- || 1号盒子 | 0.5 | 0.5 || 2号盒子 | 0.4 | 0.6 || 3号盒子 | 0.7 | 0.3 |</code></pre><script type="math/tex; mode=display">B=\left(\begin{array}{ll}0.5 & 0.5 \\0.4 & 0.6 \\0.7 & 0.3\end{array}\right)</script><ul><li>在每一个盒子抽完一次球（并放回）后，就会有概率转移到别的盒子（或自己）继续进行抽取。假设某一次是从1号盒子抽取的，下一次从一号盒子抽取的概率是0.5，下一次从二号盒子抽取的概率是0.2，下一次从3号盒子抽取的概率是0.3。我们就得到了从1号盒子出发的状态转移概率（0.5  0.2  0.3），同理假设从2号3号转移到下一个盒子，就得到了<strong>状态转移矩阵</strong></li></ul><pre><code>|  | 1号盒子 | 2号盒子 | 3号盒子 || --- | --- | --- | --- || 1号盒子 | 0.5 | 0.2 | 0.3 || 2号盒子 | 0.3 | 0.5 | 0.2 || 3号盒子 | 0.2 | 0.3 | 0.5 |</code></pre><script type="math/tex; mode=display">A=\left(\begin{array}{lll}0.5 & 0.2 & 0.3 \\0.3 & 0.5 & 0.2 \\0.2 & 0.3 & 0.5\end{array}\right)</script><h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>隐马尔可夫模型由<strong>状态序列</strong>、<strong>观测序列</strong>和<strong>两个概率矩阵</strong>组成。状态序列是隐含的、不可见的，而观测序列是可观测的。隐马尔可夫模型的基本假设是，观测序列取决于对应的状态序列，在给定状态序列的情况下，观测序列的概率分布是已知的，并且状态之间的转移满足马尔可夫性质。</p><p>首先我们假设<strong>Q是所有可能的隐藏状态的集合</strong>，<strong>V是所有可能的观测状态的集合</strong></p><script type="math/tex; mode=display">Q=\left\{q_{1}, q_{2}, \ldots, q_{N}\right\}, V=\left\{v_{1}, v_{2}, \ldots v_{M}\right\}</script><p>N是可能的隐藏状态数，M是所有的可能的观察状态数。对于长度为T的序列，I是对应的<strong>隐藏状态序列</strong>，Q是对应的<strong>观测序列</strong></p><script type="math/tex; mode=display">I=\left\{i_{1}, i_{2}, \ldots, i_{T}\right\}, O=\left\{o_{1}, o_{2}, \ldots o_{T}\right\}，i_{t} \in Q，o_{t}\in V</script><h2 id="两个基本假设"><a href="#两个基本假设" class="headerlink" title="两个基本假设"></a>两个基本假设</h2><ol><li><p>齐次马尔可夫性假设（一阶马尔可夫性假设）</p><p> 假设隐藏的马尔科夫链在任意时刻的状态<strong>只依赖于前一时刻的状态</strong>，与其他时刻的状态及观测时间 $t$ 无关，如果在时刻 $t$ 的隐藏状态是   $i_{t}=q_{i}$ ，在时刻  $t+1$ 的隐藏状态是 $i_{ t+1 }=q_{j}$ ，则从时刻 $t$ 到时刻  $t+1$ 的HMM状态转移概率 $a_{i j}$ 可以表示为:</p><script type="math/tex; mode=display"> a_{i j}=P\left(i_{t+1}=q_{j} \mid i_{t}=q_{i}\right)</script><p> 这样 $a_{i j}$ 可以组成<strong>状态转移矩阵</strong> A</p><script type="math/tex; mode=display"> A=[a_{i j}]_{N * N}</script></li><li><p>观测独立性假设</p><p> 假设任意时刻的<strong>观察状态</strong>只<strong>仅仅</strong>依赖于<strong>当前时刻的隐藏状态</strong>，这也是一个为了简化模型的假设，如果在时刻 $t$ 的隐藏状态是   $i_{t}=q_{i}$ ，对应的观察状态为 $o_{t}=v_{k}$ ,则该时刻观测状态$v_{k}$在隐藏状态$q_{j}$下生成的概率 $b_j(k)$ 为</p><script type="math/tex; mode=display"> b_{j}(k)=P(o_{t}=v_{k}|i_{t}=q_{t})</script><p> 这样 $b_j(k)$ 可以组成<strong>观测概率矩阵</strong> B</p><script type="math/tex; mode=display"> B=[b_{j}(k)]_{N * M}</script><p> 除此以外，我们需要一组在初始时刻下的隐藏状态概率分布 $\pi$</p><script type="math/tex; mode=display"> \pi =[\pi(i)]_{N} , \pi(i)=P(i_{1}=q_{i})</script><p> 因此，一个HMM模型由三元组 $\lambda$ 构成 , $\lambda =(A ,B,\pi)$，观察测序列生成过程如下图</p><p> <img src="/2023/05/17/HMM/Untitleds.png" alt="Untitled"></p><h1 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h1><h2 id="概率计算问题（评估问题）"><a href="#概率计算问题（评估问题）" class="headerlink" title="概率计算问题（评估问题）"></a>概率计算问题（评估问题）</h2><p> 给定模型$\lambda =(A,B,\pi)$ 和观测序列 $O$,求在该模型下观测序列出现的概率 $P(O|\lambda)$</p><p> 本质可以理解为：在这个$\lambda$模型下，在所有生成的观测序列中求题给的观测序列出现的概率</p><p> 因此可以直接通过<strong>暴力计算算出。</strong></p><h3 id="暴力计算"><a href="#暴力计算" class="headerlink" title="暴力计算"></a>暴力计算</h3><p> 核心：通过枚举遍历所有的隐藏状态序列，再求出<strong>在所有隐藏状态序列</strong>中<strong>生成的观测序列</strong>为<strong>O</strong>的序列概率，即求:$P(O,I|\lambda)$</p><script type="math/tex; mode=display"> I=(i_{1},i_{2},...,i_{T}),i\in Q</script><p> 其中Q有N种状态，i一共有N种取值，则序列I一共有 $T\times N^{T}$ 种可能</p><p> 思路：先<strong>求隐藏状态序列</strong>的生成概率 $P(I|\lambda)$ ，再求<strong>从状态序列生成观测序列的概率</strong>（就是乘以矩阵B） $P(O|I,\lambda)$，通过联合概率公式得</p><script type="math/tex; mode=display"> P(O,I|\lambda)=P(O|I,\lambda)P(I|\lambda)</script><p> 这就求出了隐藏状态序列 $I=(i_{1},i_{2},…,i_{T})$ 生成观测序列 $O$ 的概率，把所有隐藏状态序列都这样求和即得该模型能生成此观测状态序列 $O$ 的概率</p><script type="math/tex; mode=display"> P(O|\lambda)=\sum_{I}^{} P(O,I|\lambda)=\sum_{I}^{} P(O|I,\lambda)P(I|\lambda)</script><h3 id="前向算法：（从初始状态，递推求要怎么走才能走到想要的结果O）"><a href="#前向算法：（从初始状态，递推求要怎么走才能走到想要的结果O）" class="headerlink" title="前向算法：（从初始状态，递推求要怎么走才能走到想要的结果O）"></a>前向算法：（从初始状态，递推求要怎么走才能走到想要的结果O）</h3><p> 核心：给定模型 $\lambda =(A,B,\pi)$ ，定义到时刻$t$部分观测序列 $o_{1},o_{2},….,o_{t}$, 且状态为$q_{i}$的概率为<strong>前向概率</strong></p><script type="math/tex; mode=display"> \alpha_{t}(i) = P(o_{1},o_{2},... ,o_{t} ,i_{t}=q_{i}|\lambda)</script><ol><li><p>计算初值（初始状态）</p><p> $\pi_{i}$ 表示初始选择的隐藏状态的概率, $b_{i}(o_{1})$ 表示每一种隐藏状态发射生成观测状态$o_{1}$的概率,$N$为隐藏状态的个数</p><script type="math/tex; mode=display"> \alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), i=1,2, \cdots, N</script></li><li><p>递推直至序列末</p><p> 对 $t=1,2,….,T-1$ :</p><script type="math/tex; mode=display"> \alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), i=1,2, \cdots, N</script><p> 其中，$\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right]$ 表示 $t+1$ 时刻第 $i$ 个隐藏状态的出现概率，由<strong>齐次马尔可夫假设</strong>，此概率由前一时刻的状态通过状态转移概率（矩阵A）而来，而前一时刻每种状态都有概率转换到此状态，<strong>故$t+1$时刻状态出现的概率为$t$时刻所有状态经状态转移之后的和，下图更为直观</strong></p><p> <img src="/2023/05/17/HMM/Untitled.png" alt="t时刻前向算法的递推过程"></p><p> t时刻前向算法的递推过程</p><p> $b_{i}(o_{t+1})$ 表示每一种隐藏状态发射生成观测状态 $o_{t+1}$ 的概率</p></li><li><p>终止递推并输出</p><p> 在序列最终时刻T，由 $\alpha_{T}(i)=P(o_{1},o_{2},…,o_{T},i_{T}=q_{i}|\lambda)$ ，所有隐藏状态出现的概率之和即为观测序列出现的概率。</p><script type="math/tex; mode=display"> P（O|\lambda)=\sum_{i=1}^{N} \alpha_{T}(i)</script></li></ol></li></ol><pre><code>### 后向算法：（知道结果O，一直递推前一步求当初是怎么走过来的）核心：给定模型 $\lambda =(A,B,\pi)$ ，定义到时刻 $t$ 且状态为 $q_&#123;i&#125;$ 的条件下，从 $t+1$ 到 $T$ 的部分观测序列 $o_&#123;t+1&#125;,o_&#123;t+2&#125;,...,o_&#123;T&#125;$ 概率为**后向概率**$$\beta_&#123;t&#125;(i)=P(o_&#123;t+1&#125;,o_&#123;t+2&#125;,...,o_&#123;T&#125;|i_&#123;t&#125;=q_&#123;i&#125;,\lambda)$$4. 初始化序列末尾t=T    $$    \beta_&#123;T&#125;(i)=1,i=1,2,....,N    $$5. 递推直至序列开始    $对t=T-1,T-2,...,1$    $$    \beta_&#123;t&#125;(i)=\sum_&#123;j=1&#125;^&#123;N&#125;a_&#123;ij&#125;b_&#123;j&#125;(o_&#123;t+1&#125;)\beta_&#123;t+1&#125;(j) ,i=1,2,3....,N     $$6. 计算求和    $$    P(O|\lambda)=\sum_&#123;i=1&#125;^&#123;N&#125;\pi_&#123;i&#125;b_&#123;i&#125;(o_&#123;1&#125;)\beta_&#123;1&#125;(i)     $$利用前向概率和后向概率的定义可以将观测序列概率统一写为$$P(O \mid \lambda)=\sum_&#123;i=1&#125;^&#123;N&#125; \sum_&#123;j=1&#125;^&#123;N&#125; \alpha_&#123;t&#125;(i) \alpha_&#123;i j&#125; b_&#123;j&#125;\left(o_&#123;t+1&#125;\right) \beta_&#123;t+1&#125;(j), t=1,2, \cdots, T-1$$### **利用前向后向概率，还可以得到关于单个状态和两个状态概率的计算公式**- 给定模型 $\lambda$ 和观测 $O$ ，在时刻 $t$ 处于状态 $q_i$ 的概率    $$    \gamma_t(i)=P(i_t=q_i|O,\lambda)    $$    则由前向后向概率定义    $$    \alpha_&#123;t&#125;(i)*\beta_&#123;t&#125;(i)=P(i_t=q_i,O|\lambda)    $$    $$    \gamma_t(i)=P(i_t=q_i|O,\lambda)=\frac&#123;P(i_&#123;t&#125;=q_&#123;i&#125;,O|\lambda)&#125;&#123;P(O|\lambda)&#125;\\    =\frac&#123;\alpha_&#123;t&#125;(i)*\beta_&#123;t&#125;(i)&#125;&#123;P(O|\lambda)&#125;=\frac&#123;\alpha_&#123;t&#125;(i)*\beta_&#123;t&#125;(i)&#125;&#123;\sum_&#123;j=1&#125;^&#123;N&#125;\alpha_&#123;t&#125;(j)*\beta_&#123;t&#125;(j)&#125;    $$- 给定模型$\lambda$和观测$O$，在时刻$t$处于状态$q_i$的概率,且在时刻$t$处于状态$q_j$的概率$$\xi_t(i,j)=P(i_t=q_i,i_&#123;t+1&#125;=q_j|O,\lambda)=\frac&#123;P(i_t=q_i,i_&#123;t+1&#125;=q_j,O|\lambda)&#125;&#123;P(O|\lambda)&#125;\\=\frac&#123;\alpha_t(i)a_&#123;ij&#125;b_j(o_&#123;t+1&#125;)\beta_&#123;t+1&#125;(j)&#125;&#123;\sum_&#123;i=1&#125;^&#123;N&#125;\sum_&#123;j=1&#125;^&#123;N&#125;\alpha_t(i)a_&#123;ij&#125;b_j(o_&#123;t+1&#125;)\beta_&#123;t+1&#125;(j)&#125;$$## 学习问题## 预测（解码）问题### 近似算法核心：在每个时刻选择该时刻最有可能出现的下一个状态，从而得到一个状态序列。![viterbi.png](HMM/viterbi.png)### 维特比算法Viterbi algorithm维特比算法是依据动态规划原理求概率最大路径的，其有一个特性：**如果在t时刻通过一个节点$t^&#123;*&#125;$，对于从$t_1$ 到$t^&#123;*&#125;$的所有路径中，这条路径必须是最优的。**下面来看看这个算法直观理解假设我们的序列$T=3$。首先从起点开始选择第一个隐藏状态A，这时我们虽然可以通过$\pi$矩阵知道各个状态转移概率，但我们无法保证选择的某一个状态（A1或A2或A3）一定是在整条最优状态序列中的。![Untitled](HMM/Untitled%201.png)所以我们继续看B状态，依据动态规划原理，如果将来的最优路径要经过B1，那么，**这条路径一定包括从起点到B1的最短路径**，否则就会有更优解替代这条最优路径。在已知概率转移矩阵的情况下，我们很容易计算并比对出一条**最优路径**，假设是$S-A_1-B_1$ 。那么其余的经过B1的路径$S-A_2-B_1 和 S-A_3-B_1$ 就会**被舍弃（这是viterbi算法相比暴力遍历所有路径节省时间复杂度的关键）**![Untitled](HMM/Untitled%202.png)同理可计算出，将来要经过B2和B3的最优路径,假设是$S-A_3-B_2 和 S-A_2-B_3$ ，这样我们就得到了从S到B1B2B3的三条最优路径。![Untitled](HMM/Untitled%203.png)接下来看经过C状态，首先看经过C1状态，如果用**暴力遍历**的话，这一轮我们至少要遍历**9*3=18**条路径才能找得到最优路径，但由于我们在前一轮已经舍弃了很多条路径，所以这一轮我们只需要从$S-A_1-B_1-C_1 和 S-A_2-B_3-C_1和 S-A_3-B_2-C_1$ 这三条路径中选择最优的路径，这大大简化了计算规模，**最终假设我们选择了$S-A_3-B_2-C_1$  这条路径为经过C1的最优路径**。同理可得经过C2和C3的最优路径![Untitled](HMM/Untitled%204.png)那么当动态规划执行到状态序列最后时，我们只需从三条序列中选择最优的一条，这一条就是我们要找的全局最优序列![Untitled](HMM/Untitled%205.png)### 直观对比：近似算法的核心是从已经选择的节点寻找最优可能的下一个节点，而viterbi算法的核心是找到从**起点**到**欲通过节点**前的最优可能路径，在遇到每下一个欲通过状态时都会考虑N个路径。![Untitled](HMM/Untitled%206.png)### 具体算法：为了实现viterbi算法，我们需要定义两个变量定义变量$\delta$ ,它的作用是求在时刻$t$状态为$q_i$的所有路径中概率最大的**值**，例如$\delta_1(2)$ 就是求$t_1$时刻所有经过状态2的路径中概率最大的值（如上图的$S-A_3-B_2$ 的值）$$\delta _t(i)=\max_&#123;i_1,i_2,..,i_&#123;t-1&#125;&#125;P(i_t=i,i_&#123;t-1&#125;,...,i_1,o_t,...,o_1|\lambda),i=1,2,....,N$$利用前向概率可以得到它的递推公式$$\begin&#123;aligned&#125;\delta_&#123;t+1&#125;(i) &amp; =\max _&#123;i_&#123;1&#125;, i_&#123;2&#125;, \cdots, i_&#123;t&#125;&#125; P\left(i_&#123;t+1&#125;=i, i_&#123;t&#125;, \cdots, i_&#123;1&#125;, o_&#123;t+1&#125;, \cdots, o_&#123;1&#125; \mid \lambda\right) \\&amp; =\max _&#123;1 \leqslant j \leqslant N&#125;\left[\delta_&#123;t&#125;(j) a_&#123;j i&#125;\right] b_&#123;i&#125;\left(o_&#123;t+1&#125;\right), \quad i=1,2, \cdots, N ; \quad t=1,2, \cdots, T-1\end&#123;aligned&#125;$$有了这个变量，我们就可以递推到最后找到概率最大的节点，但是光有值还不够，我们还需要完整的一条状态序列。这时就需要viterbi算法来**从后向前回溯寻找序列节点**，定义变量$\Psi$ $$\psi_t(i)=\arg\max_&#123;1\le j\le N&#125;\left[\delta_&#123;t-1&#125;(j)a_&#123;ji&#125;\right],i=1,2,...,N$$1. 初始化    $$    \delta_1(i)=\pi_ib_i(o_1),i=1,2,3...,N\\    \psi_1(i)=0,i=1,2,3...,N    $$2. 递推计算每个时刻t经过状态i的概率最大值（N表示隐藏状态数，即对每个隐藏状态都计算一条路径）    对$t=2,3...,T$    $$     \delta_t(i)=\max _&#123;1 \leqslant j \leqslant N&#125;\left[\delta_&#123;t-1&#125;(j) a_&#123;j i&#125;\right] b_&#123;i&#125;\left(o_&#123;t&#125;\right), \quad i=1,2, \cdots, N \\    \psi_t(i)=\arg\max_&#123;1\le j\le N&#125;\left[\delta_&#123;t-1&#125;(j)a_&#123;ji&#125;\right],i=1,2,...,N    $$3. 终止    $$    P^&#123;*&#125;=\max_&#123;1\le i\le N&#125;\delta_T(i)\\    i^*_T=\arg\max_&#123;1\le i\le N&#125;[\delta_T(i)]    $$4. 回溯最优路径节点    对$t=T-1,T-2,...,1$    $$    i_t^*=\psi_&#123;t+1&#125;(i_&#123;t+1&#125;^*)    $$    可得最优路径$I^*=(i_1^*,i_2^*,...,i_T^*)$# 例题计算[HMM习题-蒋子龙](https://www.notion.so/HMM-d381c07ee64f4783b7b037bed10f20b9)[3 中文分词 - 隐马尔科夫模型分词](https://www.notion.so/3-4576f825051f4d06b0d97ee5b08b99e1)</code></pre>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
