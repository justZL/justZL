<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>全连接神经网络图像识别</title>
      <link href="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="手写神经网络手写数字识别—MINIST"><a href="#手写神经网络手写数字识别—MINIST" class="headerlink" title="手写神经网络手写数字识别—MINIST"></a>手写神经网络手写数字识别—MINIST</h1><script type="math/tex; mode=display">\textsf{@Justzl}</script><h1 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h1><ul><li>设计并实现一个简单的图像分类器，理解基本的图像识别流程及数据驱动的方法（训练、预测等阶段）</li><li>理解<strong>训练集/验证集/测试集</strong>的数据划分，以及如何使用验证数据调整模型的超参数</li><li>实现一个<strong>Softmax</strong>分类器</li><li>实现一个<strong>全连接神经网络</strong>分类器</li><li>理解不同的分类器之间的区别，以及使用不同的更新方法优化神经网络</li><li><strong>尝试使用不同的损失函数和正则化方法，观察并分析其对实验结果的影响</strong></li><li><strong>尝试使用不同的优化算法</strong>，观察并分析其对训练过程和实验结果的影响 (如batch GD, online GD, mini-batch GD, SGD, 或其它的优化算法，如Momentum, Adagrad, Adam, Admax)</li></ul><h1 id="任务思路"><a href="#任务思路" class="headerlink" title="任务思路"></a>任务思路</h1><ol><li>理解基本的图像识别流程及数据驱动的方法</li><li>利用numpy搭建一个soft分类器和简单神经网络，实现神经网络的基本功能</li><li>实现并探究不同<strong>初始化方法、优化算法、损失函数、正则化、激活函数</strong>对实验结果的影响</li><li>实现并探究不同<strong>迭代次数、学习率、训练/验证集划分</strong>对实验结果的影响</li></ol><h1 id="实现框架图"><a href="#实现框架图" class="headerlink" title="实现框架图"></a>实现框架图</h1><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled.png" alt="Untitled"></p><h1 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h1><p>图像识别流程通常包括以下几个基本阶段</p><ul><li><p>数据收集和准备阶段</p><ol><li>数据收集</li><li>数据标注</li></ol></li><li><p>数据预处理阶段</p><ol><li>图像调整</li><li>归一化</li><li>数据增强</li></ol></li><li><p>模型选择阶段</p><ol><li>选择模型</li><li>预训练</li></ol></li><li><p>模型训练阶段</p><ol><li>最小化损失函数</li><li>优化算法</li></ol></li><li><p>模型评估和调优阶段</p><ol><li>验证集</li><li>调参</li></ol></li><li><p>模型预测阶段</p></li></ul><h1 id="任务实现"><a href="#任务实现" class="headerlink" title="任务实现"></a>任务<strong>实现</strong></h1><h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a><strong>数据导入</strong></h2><p><strong>读取data目录下的minist数据集，将其转换成（6000，28，28）的矩阵，并展示其中一张图片如下</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_mnist_images</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        data = np.frombuffer(file.read(), np.uint8, offset=<span class="number">16</span>)</span><br><span class="line">    data = data.reshape(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).astype(np.float32)</span><br><span class="line">    <span class="keyword">return</span> data / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_mnist_labels</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        data = np.frombuffer(file.read(), np.uint8, offset=<span class="number">8</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%201.png" alt="Untitled"></p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><strong>数据预处理</strong></h2><p>将读取的数据转换为长784的一维向量，并将训练/测试集的输出真实值转换为独热编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转换一维向量</span></span><br><span class="line">X_train=train_images.reshape(<span class="number">60000</span>,<span class="number">784</span>).astype(<span class="string">&#x27;float&#x27;</span>)/<span class="number">255.0</span></span><br><span class="line">X_test=test_images.reshape(<span class="number">10000</span>,<span class="number">784</span>).astype(<span class="string">&#x27;float&#x27;</span>)/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#独热编码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">up_onehot</span>(<span class="params">labels,num</span>):</span><br><span class="line">    labelhot=np.zeros((labels.shape[<span class="number">0</span>],num))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labels.shape[<span class="number">0</span>]):</span><br><span class="line">        labelhot[i,labels[i]]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> labelhot</span><br><span class="line">Y_train=up_onehot(train_labels,<span class="number">10</span>)</span><br><span class="line">Y_test=up_onehot(test_labels,<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%202.png" alt="Untitled"></p><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><h3 id="softmax分类器"><a href="#softmax分类器" class="headerlink" title="softmax分类器"></a>softmax分类器</h3><p>将输入层的784个输入直接连接到输出层的十个输出,经过一次参数加权再用softmax分类</p><blockquote><p>代码详见softmax.py</p><ul><li>训练</li></ul></blockquote><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">X_train,Y_train,learning_rate</span>):</span><br><span class="line">    som=Output.shape[<span class="number">0</span>]</span><br><span class="line">    Output=softmax(np.dot(X_train,W)+B)</span><br><span class="line">    loss=-np.<span class="built_in">sum</span>(Y_train * np.log(Output)) / som</span><br><span class="line">    delta=Output-Y_train</span><br><span class="line">    W=W-learning_rate * (np.dot(Output.T, delta)/som) </span><br><span class="line">    B=B-learning_rate * np.<span class="built_in">sum</span>(delta, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>) / som</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></code></pre><ul><li><p>预测</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X_test,Y_test</span>):</span><br><span class="line">    Output=softmax(np.dot(X_test,W)+B)</span><br><span class="line">    num=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Output.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> np.argmax(Output[i])==np.argmax(Y_test[i]):</span><br><span class="line">            num+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> num/Output.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></li></ul><p>取不同学习率下的softmax进行学习和预测，可见softmax分类器在拟合非线性数据时效果不好</p><div class="table-container"><table><thead><tr><th>学习率</th><th>准确率</th></tr></thead><tbody><tr><td>0.5</td><td>80.1%</td></tr><tr><td>0.1</td><td>68.69%</td></tr><tr><td>0.05</td><td>59.81%</td></tr><tr><td>0.01</td><td>17.79%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/softmaz.png" alt="softmaz.png"></p><h3 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h3><blockquote><p>代码详见network.py</p><ul><li>定义激活函数</li></ul></blockquote><pre><code>![Untitled](Neuralnet_picture/Untitled%203.png)</code></pre><ul><li><p>搭建神经网络<strong>Feedforwardnetwork</strong>（512<em>512</em>10）</p><p>  搭建全连接神经网络，L表示层数，<strong>batchsize</strong>表示梯度下降时每个batch的大小，<strong>Mlist</strong>表示每一层的神经元数量。</p><ol><li><p>参数初始化</p><p> 随机初始化，Xiver初始化和He初始化，三者选其一</p></li><li><p>前向传播</p><p> 前两层可以选用其他更多激活函数，最后一层使用softmax进行分类</p></li><li><p>反向传播</p></li></ol></li></ul><h2 id="模型训练-amp-预测"><a href="#模型训练-amp-预测" class="headerlink" title="模型训练&amp;预测"></a>模型训练&amp;预测</h2><p>在输出层，我们使用 softmax 激活函数并假设使用交叉熵损失（Cross-Entropy Loss），我们首先计算损失函数相对于输出层的输出的梯度，对于第 i 个神经元，根据均方误差或交叉熵的导数规则，我们有</p><script type="math/tex; mode=display">L = \frac{1}{2} \sum_{i=1}^{C} (y_i - a^{(3)}_i)^2\\L = -\sum_{i=1}^{C} y_i \log(a^{(3)}_i)\\\frac{\partial L}{\partial A^{(3)}} = A^{(3)} - Y</script><p>接下来，我们使用链式法则，将这个梯度传播到输出层的输出上：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial Z^{(3)}} = \frac{\partial L}{\partial A^{(3)}} \cdot \frac{\partial A^{(3)}}{\partial Z^{(3)}}</script><p>对于 softmax 激活函数，$\frac{\partial A^{(3)}}{\partial Z^{(3)}}$ 是雅可比矩阵，可以表示为：</p><script type="math/tex; mode=display">\frac{\partial A^{(3)}}{\partial Z^{(3)}} = A^{(3)} \cdot (I - A^{(3)})</script><p>得到参数W偏导</p><script type="math/tex; mode=display">\frac{\partial L}{\partial W^{(3)}} = A^{(2)T} \cdot \frac{\partial L}{\partial Z^{(3)}}</script><p>同理可得其余参数偏导</p><script type="math/tex; mode=display">\frac{\partial L}{\partial W^{(2)}} = A^{(1)T} \cdot \frac{\partial L}{\partial Z^{(2)}}\\\frac{\partial L}{\partial W^{(1)}} = X^T \cdot \frac{\partial L}{\partial Z^{(1)}}</script><ul><li><p>交叉验证训练</p><p>  <img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%204.png" alt="采取交叉验证的方式训练"></p><p>  采取交叉验证的方式训练</p></li><li><p>训练</p><p>  <img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%205.png" alt="不采用交叉验证的方式训练"></p><p>  不采用交叉验证的方式训练</p><p>  <img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/d.png" alt="d.png"></p><ul><li><p>随机梯度下降</p><p>  <img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%206.png" alt="Untitled"></p></li></ul></li><li><p>预测</p><p>  <img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%207.png" alt="Untitled"></p></li></ul><h1 id="实验结果对比"><a href="#实验结果对比" class="headerlink" title="实验结果对比"></a>实验结果对比</h1><h2 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h2><p>取层数为三层（不含输入层），层中<strong>神经元数量</strong>为512<em>512</em>10，<strong>激活函数</strong>为leakyrelu函数，<strong>学习率</strong>为0.05，交叉验证<strong>折数</strong>为5，<strong>batch</strong>大小为600，<strong>损失函数</strong>为交叉熵损失，<strong>无正则化</strong>和<strong>无dropout</strong>的网络为<strong>默认网络</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">nenetwork=Feedforwardnetwork(<span class="number">3</span>,[<span class="number">784</span>,<span class="number">512</span>,<span class="number">512</span>,<span class="number">10</span>],<span class="string">&#x27;H&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;初始ACC&quot;</span>,nenetwork.predict(X_test,Y_test,pfx=Leakyrelu)*<span class="number">100</span>,<span class="string">&quot;%&quot;</span>)</span><br><span class="line">Outx=nenetwork.train_no_cross(                  <span class="comment">#可选是否采用交叉验证，train为使用交叉验证</span></span><br><span class="line">                    X_train,Y_train,            <span class="comment">#读入的训练集输入和输出</span></span><br><span class="line">                    counts=<span class="number">200</span>,                 <span class="comment">#迭代次数</span></span><br><span class="line">                    trainfolds=<span class="number">5</span>,               <span class="comment">#交叉验证折数</span></span><br><span class="line">                    learning_rate=<span class="number">0.05</span>,         <span class="comment">#学习率</span></span><br><span class="line">                    batch_size=<span class="number">600</span>,              <span class="comment">#batch大小</span></span><br><span class="line">                    lossfunction=cross_entropy, <span class="comment">#损失函数 (mean_square)</span></span><br><span class="line">                    reg_strength=<span class="number">0</span>,             <span class="comment">#L2正则化强度</span></span><br><span class="line">                    dropout_prob=<span class="number">0</span>,             <span class="comment">#dropout概率</span></span><br><span class="line">                    func=Leakyrelu,             <span class="comment">#激活函数</span></span><br><span class="line">                    dfunc=Leakyrelu_derivative, <span class="comment">#激活函数导数，要对应使用</span></span><br><span class="line">adam=<span class="literal">False</span>,                 <span class="comment">#是否使用adam梯度更新</span></span><br><span class="line">                    lrdecay=<span class="number">1</span>)               <span class="comment">#学习率衰减率1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 最终ACC:&quot;</span>,nenetwork.predict(X_test,Y_test,pfx=Leakyrelu)*<span class="number">100</span>,<span class="string">&quot;%&quot;</span>)</span><br></pre></td></tr></table></figure><p>采用交叉验证的方式训练，每次训练4折数据，预测1折数据，分别取5次折预测准确率和loss值的最高值作为最终训练的结果，在迭代200次后最高准确率为<strong>90.21%</strong></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%208.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%209.png" alt="Untitled"></p><h2 id="网络规模"><a href="#网络规模" class="headerlink" title="网络规模"></a>网络规模</h2><p>对比不同神经元规模的网络，使用<strong>默认网络</strong>进行学习和预测</p><div class="table-container"><table><thead><tr><th>网络规模</th><th>学习率</th><th>迭代次数</th><th>loss值</th><th>验证集准确率</th><th>测试集准确率</th></tr></thead><tbody><tr><td>256<em>256</em>10</td><td>0.05</td><td>200</td><td>0.2667</td><td>90.23%</td><td>89.29%</td></tr><tr><td>512<em>512</em>10</td><td>0.05</td><td>200</td><td>0.2639</td><td>90.43%</td><td>90.21%</td></tr><tr><td>768<em>768</em>10</td><td>0.05</td><td>200</td><td>0.2557</td><td>90.74%</td><td>90.69%</td></tr><tr><td>1024<em>1024</em>10</td><td>0.05</td><td>200</td><td>0.0689</td><td>91.06%</td><td>90.66%</td></tr><tr><td>512<em>256</em>10</td><td>0.05</td><td>200</td><td>0.2595</td><td>90.48%</td><td>89.68%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2010.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2011.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2012.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2013.png" alt="Untitled"></p><h2 id="学习率和迭代次数"><a href="#学习率和迭代次数" class="headerlink" title="学习率和迭代次数"></a>学习率和迭代次数</h2><p>选取leakyrelu函数为激活函数（缓解使用ReLU神经元死亡问题），超参数设置为0.01，网络规模为512<em>512</em>10，分别选取一系列学习率迭代200次查看训练集和测试集的准确率</p><div class="table-container"><table><thead><tr><th>激活函数</th><th>激活函数超参数</th><th>lr</th><th>迭代100次T-ACC</th><th>迭代200次T-ACC</th></tr></thead><tbody><tr><td>LeakyRelu</td><td>0.01</td><td>0.5</td><td>52.10%</td><td>67.74%</td></tr><tr><td>LeakyRelu</td><td>0.01</td><td>0.1</td><td>83.97%</td><td>86.06 %</td></tr><tr><td>LeakyRelu</td><td>0.01</td><td>0.08</td><td>88.53%</td><td>80.58%</td></tr><tr><td>LeakyRelu</td><td>0.01</td><td>0.05</td><td>87.14%</td><td>90.25%</td></tr><tr><td>LeakyRelu</td><td>0.01</td><td>0.016</td><td>60.41%</td><td>81.22%</td></tr><tr><td>LeakyRelu</td><td>0.01</td><td>0.00986</td><td>46.73%</td><td>75.52%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2014.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2015.png" alt="Untitled"></p><p>由图片和数据可以得出</p><ul><li>学习率设置为0.05为以上几个网络的<strong>最优值</strong></li><li>学习率为0.5的网络出现较大幅度的震荡，模型收敛最慢，<strong>准确率最低</strong>；而学习率为0.016的网络最平滑，但模型收敛依然慢，<strong>欠拟合</strong>现象显著</li></ul><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>设置两个网络均为0.3学习率，其中一个使用学习率衰减，迭代200次后如下</p><div class="table-container"><table><thead><tr><th>学习率</th><th>lrdecay</th><th>train-acc</th><th>test-acc</th></tr></thead><tbody><tr><td>0.3</td><td>1</td><td>92.35%</td><td>89.75%</td></tr><tr><td>0.3</td><td>0.98</td><td>90.94%</td><td>91.7%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/lrdecay.png" alt="lrdecay.png"></p><p>由图片和数据可以看出</p><ul><li>未使用学习率衰减机制的神经网络在训练时<strong>loss值波动很大</strong>，使用衰减机制的神经网络loss值则收敛的<strong>较为平滑。</strong></li><li>未使用衰减的神经网络出现了<strong>过拟合</strong>现象，而 使用学习率衰减，缓解了学习率过大引发的过拟合现象。</li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_square</span>(<span class="params">Y_train,A3,<span class="built_in">sum</span></span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.mean((A3 - Y_train) ** <span class="number">2</span>) / <span class="built_in">sum</span></span><br></pre></td></tr></table></figure><h3 id="多分类交叉熵"><a href="#多分类交叉熵" class="headerlink" title="多分类交叉熵"></a>多分类交叉熵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">Y_train,A3,<span class="built_in">sum</span></span>):</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(Y_train * np.log(A3)) / <span class="built_in">sum</span></span><br></pre></td></tr></table></figure><p>使用不同损失函数迭代200次后结果如下图，最终Test预测准确率均为91.72%</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2016.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2017.png" alt="Untitled"></p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="使用sigmoid函数和tanh函数"><a href="#使用sigmoid函数和tanh函数" class="headerlink" title="使用sigmoid函数和tanh函数"></a>使用sigmoid函数和tanh函数</h3><p>s型激活函数在次多分类问题上效果不好,且与softmax有所冲突</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2018.png" alt="Untitled"></p><h3 id="使用LeakyRelu函数"><a href="#使用LeakyRelu函数" class="headerlink" title="使用LeakyRelu函数"></a>使用LeakyRelu函数</h3><p>调整leakyrelu函数不同超参数，发现其对结果影响不大，初步推测原因是训练数据较简单</p><div class="table-container"><table><thead><tr><th>激活函数</th><th>激活函数超参数</th><th>lr</th><th>最小loss值</th><th>迭代200次Test-ACC</th></tr></thead><tbody><tr><td>LeakyRelu</td><td>0.9</td><td>0.05</td><td>0.057403</td><td>90.32%</td></tr><tr><td>LeakyRelu</td><td>0.5</td><td>0.05</td><td>0.0928927</td><td>91.74%</td></tr><tr><td>LeakyRelu</td><td>0.1</td><td>0.05</td><td>0.059380</td><td>92.75%</td></tr><tr><td>LeakyRelu</td><td>0.01</td><td>0.05</td><td>0.058603</td><td>93.33%</td></tr><tr><td>LeakyRelu</td><td>0.001</td><td>0.05</td><td>0.057936</td><td>92.39%</td></tr><tr><td>LeakyRelu</td><td>0.0001</td><td>0.05</td><td>0.049473</td><td>92.36%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2019.png" alt="Untitled"></p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>L2正则化通过向损失函数添加权重的平方和来实现，以鼓励权重值趋向于较小的值。</p><p>L2正则化的目标是<strong>最小化损失函数</strong>L，同时<strong>考虑权重的平方和</strong>。神经网络的损失函数通常包括两个部分：数据项和正则化项。数据项衡量模型对训练数据的拟合程度，而正则化项衡量权重的大小。L2正则化推导如下</p><script type="math/tex; mode=display">R(w) = \frac{\lambda}{2} \sum_{i=1}^{N} w_i^2\\\mathcal{L}=\mathcal{L}+R(w)=\mathcal{L}+\frac{\lambda}{2} \sum_{i=1}^{N}\\w_i = w_i - \alpha \left(\frac{\partial \mathcal{L}}{\partial w_i} + \lambda w_i\right)</script><p>设置学习率同为0.08的神经网络使用不同强度的正则化结果如下：</p><div class="table-container"><table><thead><tr><th>激活函数</th><th>lr</th><th>reg_strength</th><th>loss是否收敛</th><th>Test-Acc</th></tr></thead><tbody><tr><td>leakyrelu : 0.1</td><td>0.08</td><td>0.5</td><td>否</td><td>10.62%</td></tr><tr><td>leakyrelu : 0.1</td><td>0.08</td><td>0.01</td><td>是</td><td>87.02%</td></tr><tr><td>leakyrelu : 0.1</td><td>0.08</td><td>0.005</td><td>是</td><td>92.19%</td></tr><tr><td>leakyrelu : 0.1</td><td>0.08</td><td>0.001</td><td>是</td><td>91.72%</td></tr><tr><td>leakyrelu : 0.1</td><td>0.08</td><td>0.0005</td><td>是</td><td>89.75%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Figure_2.png" alt="Figure_2.png"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Figure_1-hhn.png" alt="Figure_1-hhn.png"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Figure_3.png" alt="Figure_3.png"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/57367.png" alt="57367.png"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5816.png" alt="5816.png"></p><p>以上5张图片为更换正则化强度的输出结果，<strong>最后一张图片忘记修改title的值，实际为0.0005</strong>。由以上数据可知</p><ul><li>使用正则化缓解了学习率为0.08的神经网络的过拟合现象（上文0.08的神经网络测试集准确率低于训练集）</li><li>正则化强度为0.001和0.005的神经网络对于数据拟合的最好，测试准确率较高，而正则化强度为0.5和0.01的神经网络则对参数惩罚过大，模型欠拟合甚至不收敛。</li></ul><h3 id="dropout正则化"><a href="#dropout正则化" class="headerlink" title="dropout正则化"></a>dropout正则化</h3><p>在前向传播中加入如下代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> dropout_prob&gt;<span class="number">0</span>:</span><br><span class="line">            mask1 = np.random.rand(*self.A1.shape) &lt;dropout_prob</span><br><span class="line">            self.A1 *= mask1 / dropout_prob</span><br></pre></td></tr></table></figure><p>这段代码首先随机生成了一个与A1维度一样的mask二进制矩阵，random取随机数小于dropoutprob的值会置为False，其余为True</p><div class="table-container"><table><thead><tr><th>激活函数</th><th>lr</th><th>drop_prob</th><th>Train-Acc</th><th>Test-Acc</th></tr></thead><tbody><tr><td>leakyrelu</td><td>0.03</td><td>0</td><td>86.0%</td><td>86.61%</td></tr><tr><td>leakyrelu</td><td>0.03</td><td>0.2</td><td>35.77%</td><td>35.79%</td></tr><tr><td>leakyrelu</td><td>0.03</td><td>0.8</td><td>88.59%</td><td>88.46%</td></tr><tr><td>leakyrelu</td><td>0.03</td><td>0.5</td><td>66.7%</td><td>67.3%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2020.png" alt="Untitled"></p><h2 id="adam梯度更新方法"><a href="#adam梯度更新方法" class="headerlink" title="adam梯度更新方法"></a>adam梯度更新方法</h2><ol><li><p>AdaGrad方法</p><p> Adagrad算法与梯度下降法的不同之处在于每次参数更新都使用<strong>累计平方梯度</strong></p><script type="math/tex; mode=display"> \delta\gets \delta+g\odot g\\\nabla \theta \gets  -\frac{\alpha }{r +\sqrt{\delta} } \odot g</script><blockquote><p>$\alpha$是全局学习率，$\delta$是累计平方梯度，$r$是一个很小的常数用于数值稳定，g是当前梯度</p></blockquote><p> 设置全局学习率之后，每次参数更新，<strong>全局学习率逐参数的除以历史梯度平方和的平方根</strong>，使得每个参数的学习率不同。</p><p> 起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度<strong>。<em>即梯度较大的参数将拥有较小的学习率，梯度较小的参数将拥有较大的学习率。（图来自Andrew Ng）</em></strong></p><p> <img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2021.png" alt="Untitled"></p></li><li><p>RMSprop方法</p><p> RMSprop与adagrad的不同在于计算累计平方梯度时<strong><em>增加了一个衰减系数来控制历史信息的获取多少</em></strong></p><script type="math/tex; mode=display"> \delta\gets \rho \times \delta+(1-\rho)\times  g\odot g\\\nabla \theta \gets  -\frac{\alpha }{\sqrt{r +\delta} } \odot g</script><blockquote><p>$\alpha$是全局学习率，$\delta$是累计平方梯度，r是一个很小的常数用于数值稳定，$\rho$为衰减率</p></blockquote></li><li><p>Adam方法</p><p> Adam 算法将Adagrad方法和RMSprop方法结合。随机梯度下降保持单一的学习率更新所有的权重，<strong>学习率在训练过程中并不会改变</strong>。而 Adam 通过计算梯度的<strong>一阶矩估计</strong>和<strong>二阶矩估计</strong>而为不同的参数设计独立的自适应性学习率。</p><script type="math/tex; mode=display"> \begin{array}{l}\boldsymbol{S}_{t}=\beta_{1} \boldsymbol{S}_{t-1}+\left(1-\beta_{1}\right) \boldsymbol{g}_{t}\\\\\boldsymbol{R}_{t}=\beta_{2} \boldsymbol{R}_{t-1}+\left(1-\beta_{2}\right) \boldsymbol{g}_{t}^{2}\end{array}</script><blockquote><p>$S_{t}$是一阶矩估计，用来储存梯度的指数平均移动，类似动量<br> $R_{t}$是二阶矩估计，用来储存梯度的平方的指数平均移动<br> $\hat{S_{t}}和\hat{R_{t}}$是偏差修正后的矩估计<br> $\beta_{1}和\beta_2$是衰减因子</p></blockquote><p> 进行偏差修正，以避免初始步骤的矩估计偏向零,并使用修正后的矩估计来更新参数</p><script type="math/tex; mode=display"> \begin{array}{l}\hat{\boldsymbol{S}}_{t}=\frac{\boldsymbol{S}_{t}}{1-\beta_{1}^{t}}\\\\\hat{\boldsymbol{R}}_{t}=\frac{\boldsymbol{R}_{t}}{1-\beta_{2}^{t}}\\\\ \theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{\boldsymbol{R}_{t}}}+r} \cdot \hat{\boldsymbol{S}_{t}}\end{array}</script><p> 这使得Adam在训练神经网络时通常能够更稳定地收敛，并且无需手动调整学习率。下图是在同时学习率为0.01的网络训练下的loss值情况：</p><p> <img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2022.png" alt="Untitled"></p><p> <img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2023.png" alt="Untitled"></p><p> 由图可知使用adam算法会使得loss下降极快，仅需迭代50次即可达到很好的预测准确率和很低的loss值（下图为使用adam算法训练50次的网络结果）</p><p> <img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2024.png" alt="Untitled"></p></li></ol><h1 id="心得体会"><a href="#心得体会" class="headerlink" title="心得体会"></a>心得体会</h1><ul><li>之前接触的神经网络都是停留在理论层面,唯一一次的使用是基于pytorch成熟的框架搭建的,本次实验从零开始搭建神经网络遇到了很多问题,比如:网络的搭建,反向传播算法的实现,各种优化算法的实现等等.前前后后历经10几天一点点搭建网络确实使得我对反向传播算法理解更通透,也正是不调用其他库才能看到网络在训练过程中遇到的各种奇奇怪怪的问题,对loss值,过拟合,欠拟合等有了更多更深入的理解和判断.</li></ul><p><a href="https://www.notion.so/A2-2020211073-d391c2c38419498390a415ad76b838d6?pvs=21">问题探究-A2-蒋子龙-2020211073</a></p><p><a href="https://www.notion.so/readme-12b995bd61e2494abed058619123f115?pvs=21">readme</a></p><p><a href="https://www.notion.so/2020211073-a76376a7e98a4edd8b0b9c33b9d0ea18?pvs=21">智能计算系统-蒋子龙-2020211073</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络图像识别</title>
      <link href="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="手写卷积神经网络图像识别——对比pytorch"><a href="#手写卷积神经网络图像识别——对比pytorch" class="headerlink" title="手写卷积神经网络图像识别——对比pytorch"></a>手写卷积神经网络图像识别——对比pytorch</h1><script type="math/tex; mode=display">\textsf{@Justzl}</script><h1 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h1><ul><li>理解卷积神经网络的基本结构、代码实现及训练过程</li><li>应用dropout和多种normalization方法，理解它们对模型泛化能力的影响</li><li>理解如何通过交叉验证，为神经网络找到最好的hyperparameters</li><li>在训练网络的过程中，可根据需要自由尝试其它提升性能的方法，例如通过增加模型层数、使用不同的正则化方法、使用模型集成</li></ul><h1 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h1><p>详见代码文件中<strong>read.md</strong></p><h1 id="数据集说明"><a href="#数据集说明" class="headerlink" title="数据集说明"></a>数据集说明</h1><p>该数据集共有60000张彩色图像，这些图像是32*32，分为10个类，每类6000张图。这里面有50000张用于训练，构成了5个训练批，每一批10000张图；另外10000用于测试，单独构成一批。测试批的数据里，取自10类中的每一类，每一类随机取1000张。抽剩下的就随机排列组成了训练批。注意一个训练批中的各类图像并不一定数量相同，总的来看训练批，每一类都有5000张图。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled.png" alt="Untitled0"></p><p>下载解压后数据格式如下，以字典形式存储。每个batch字典包含<strong>data</strong>和<strong>labels</strong>，其中<strong>data</strong>是10000<em>3072的数组，每1024列为图片R/G/B通道，其数据组织为**10000张3通道的32</em>32像素图片batchmeta<strong>中包含的是labels与实际对应的类名，使用右图两个方法来</strong>解包<strong>数据和进行</strong>独热编码**</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%201.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%202.png" alt="Untitled"></p><p>下图是训练batch1中的数据和batchesmeta中的字典，右图为训练集和测试集中的某一张图片。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%203.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%204.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%205.png" alt="Untitled"></p><h1 id="基于numpy手写卷积神经网络"><a href="#基于numpy手写卷积神经网络" class="headerlink" title="基于numpy手写卷积神经网络"></a>基于numpy手写卷积神经网络</h1><h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h2><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%206.png" alt="Untitled"></p><p>上图为导入全部的训练数据（测试集同理），将所有数据导入后，得到4个数组，规模如下，3代表rgb通道值。</p><div class="table-container"><table><thead><tr><th>数据集</th><th>规模</th></tr></thead><tbody><tr><td>train_data</td><td>50000<em>3</em>32*32</td></tr><tr><td>train_labels</td><td>50000<em>3</em>32*32</td></tr><tr><td>test_data</td><td>10000<em>3</em>32*32</td></tr><tr><td>test_labels</td><td>10000<em>3</em>32*32</td></tr></tbody></table></div><h2 id="定义卷积神经网络"><a href="#定义卷积神经网络" class="headerlink" title="定义卷积神经网络"></a>定义卷积神经网络</h2><p>考虑较为简单的卷积神经网络，结构入下图所示，只考虑一层卷积，一层池化的情况</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%207.png" alt="Untitled"></p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>考虑通用卷积操作，假设输入为<strong>3<em>32</em>32</strong>，卷积核为<strong>3<em>2</em>2*2</strong>，则输出为<strong>2<em>16</em>16</strong></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%208.png" alt="Untitled"></p><p>首先想到的是对于<strong>每一个样本每一个通道的每一个行和列取与卷积核大小相同的数据做运算</strong>，但是这样效率实在是太低，计算很慢。（如下代码）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convolute</span>(<span class="params">X,W,Sw,Sh</span>):</span><br><span class="line">    Xw=X.shape[<span class="number">2</span>]</span><br><span class="line">    Xh=X.shape[<span class="number">3</span>]</span><br><span class="line">    Ww=W.shape[<span class="number">2</span>]</span><br><span class="line">    Wh=W.shape[<span class="number">3</span>]</span><br><span class="line">    Yw=<span class="built_in">int</span>((Xw-Ww)/Sw )+<span class="number">1</span></span><br><span class="line">    Yh=<span class="built_in">int</span>((Xh-Wh)/Sh )+<span class="number">1</span></span><br><span class="line">    Y=np.zeros((X.shape[<span class="number">0</span>], W.shape[<span class="number">1</span>],Yw ,Yh))</span><br><span class="line">    <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span> (X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span> (W.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[rank][y]=(signal.convolve2d(X[rank][<span class="number">0</span>],W[<span class="number">0</span>][y],mode=<span class="string">&#x27;valid&#x27;</span>, boundary=<span class="string">&#x27;wrap&#x27;</span>)[::Sh, ::Sw]</span><br><span class="line">                                      +signal.convolve2d(X[rank][<span class="number">1</span>],W[<span class="number">1</span>][y],mode=<span class="string">&#x27;valid&#x27;</span>, boundary=<span class="string">&#x27;wrap&#x27;</span>)[::Sh, ::Sw]</span><br><span class="line">                                      +signal.convolve2d(X[rank][<span class="number">2</span>],W[<span class="number">2</span>][y],mode=<span class="string">&#x27;valid&#x27;</span>, boundary=<span class="string">&#x27;wrap&#x27;</span>)[::Sh, ::Sw])</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>故采用<strong>矩阵相乘</strong>的方式来计算卷积。先考虑最简单的一次卷积运算，如下图，由图可知<strong>输入数据与卷积核卷积等价于将输入数据展平与卷积核展平后的转置矩乘</strong>。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%209.png" alt="Untitled"></p><p>故要将卷积核与一整个输入进行卷积，则先将与卷积核同维度的窗口在输入数据上滑动，每次<strong>将滑动得到的矩阵展平并存储下来</strong>，得到一个<strong>Kh*Km的矩阵K。</strong>这里的K的形状为<strong>[16*16,4]。</strong>则对于每个32*32的输入图像，都可通过滑动求得其K矩阵</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%25E5%25BE%25AE%25E4%25BF%25A1%25E5%259B%25BE%25E7%2589%2587_20231029153736.jpg" alt="微信图片_20231029153736.jpg"></p><p>现在我们要将3通道的输入数据和3通道的卷积核矩乘，则对于每个通道，先滑动得到K矩阵，再将K矩阵和卷积核矩乘，得到16*16的向量，将其展开为二维矩阵即为输出矩阵。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2010.png" alt="Untitled"></p><p>再对第二层的三个卷积核执行同样的操作，即可得到2<em>16</em>16的输出矩阵</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2011.png" alt="Untitled"></p><p>用代码实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convolute</span>(<span class="params">X,W,Sh,Sw,bias</span>):</span><br><span class="line">    Xh=X.shape[<span class="number">2</span>]</span><br><span class="line">    Xw=X.shape[<span class="number">3</span>]</span><br><span class="line">    Wh=W.shape[<span class="number">2</span>]</span><br><span class="line">    Ww=W.shape[<span class="number">3</span>]</span><br><span class="line">    Yh=<span class="built_in">int</span>((Xh-Wh)/Sh )+<span class="number">1</span></span><br><span class="line">    Yw=<span class="built_in">int</span>((Xw-Ww)/Sw )+<span class="number">1</span></span><br><span class="line">    <span class="comment">#K为将卷积核在输入数据上滑动的矩阵依次展开并展平</span></span><br><span class="line">    K=np.zeros((X.shape[<span class="number">0</span>],X.shape[<span class="number">1</span>],Yw*Yh,Ww*Wh))</span><br><span class="line">    Y=np.zeros((X.shape[<span class="number">0</span>], W.shape[<span class="number">1</span>],Yh*Yw,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]): <span class="comment">#对每一个样本</span></span><br><span class="line">        <span class="keyword">for</span> rgb <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">            temp =<span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Yh):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Yw):</span><br><span class="line">                    sample=X[rank,rgb,i*Sh:i*Sh+Wh,j*Sw:j*Sw+Ww]</span><br><span class="line">                    K[rank,rgb,temp]=sample.flatten()</span><br><span class="line">                    temp+=<span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#每个通道的k计算后与卷积核相乘</span></span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(W.shape[<span class="number">1</span>]):</span><br><span class="line">                Y[rank,m]=Y[rank,m]+np.dot(K[rank,rgb],W[rgb,m].reshape((Wh*Ww,<span class="number">1</span>)))+bias[m]</span><br><span class="line">    </span><br><span class="line">    Yout=Y.reshape((Y.shape[<span class="number">0</span>],Y.shape[<span class="number">1</span>],Yh,Yw))</span><br><span class="line">    <span class="keyword">return</span> Yout,K</span><br></pre></td></tr></table></figure><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>采用最大池化的方式计算，每次只取滑动窗口中最大的值为保留值，并记录下该值<strong>位于窗口中的具体位置到Mcin</strong>，其余值舍弃。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_pooling</span>(<span class="params">X,pool_size=(<span class="params"><span class="number">2</span>,<span class="number">2</span></span>)</span>):</span><br><span class="line">    ph,pw=pool_size</span><br><span class="line">    Mcin=np.zeros_like(X)</span><br><span class="line">    X_copy=X</span><br><span class="line"><span class="comment">#若数据规模无法被滑动窗口整除，则在矩阵周围补上与滑动窗口大小相同的几层0</span></span><br><span class="line">    <span class="keyword">if</span> X_copy.shape[<span class="number">2</span>] % ph !=<span class="number">0</span>:</span><br><span class="line">        plusw=np.zeros((X_copy.shape[<span class="number">0</span>],X_copy.shape[<span class="number">1</span>],ph-X_copy.shape[<span class="number">2</span>] % ph,X_copy.shape[<span class="number">3</span>]))</span><br><span class="line">        X_copy=np.concatenate((X_copy,plusw), axis=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span>  X.shape[<span class="number">3</span>] % pw !=<span class="number">0</span>:</span><br><span class="line">        plush=np.zeros((X_copy.shape[<span class="number">0</span>],X_copy.shape[<span class="number">1</span>],X_copy.shape[<span class="number">2</span>],pw-X_copy.shape[<span class="number">3</span>] % pw))</span><br><span class="line">        X_copy=np.concatenate((X_copy,plush), axis=<span class="number">3</span>)</span><br><span class="line">    <span class="comment">#计算输出形状时向下取整取，确保利用上了所有数据并且不溢出</span></span><br><span class="line">outh=X_copy.shape[<span class="number">2</span>]//ph</span><br><span class="line">    outw=X_copy.shape[<span class="number">3</span>]//pw</span><br><span class="line">    Out=np.zeros((X.shape[<span class="number">0</span>],X.shape[<span class="number">1</span>],outh,outw))</span><br><span class="line">    <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span> (X_copy.shape[<span class="number">0</span>]): <span class="comment">#对每一个样本</span></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span> (X_copy.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (outh):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(outw):</span><br><span class="line">                    window = X_copy[rank][y][i * ph:(i + <span class="number">1</span>) * ph, j * pw:(j + <span class="number">1</span>) * pw]</span><br><span class="line">                    max_index = np.unravel_index(np.argmax(window), window.shape)</span><br><span class="line">                    <span class="comment">#将最大值记录，并保留位置信息</span></span><br><span class="line">                    Out[rank][y][i][j]=np.<span class="built_in">max</span>(window)</span><br><span class="line">                    Mcin[rank][y][i * ph+max_index[<span class="number">0</span>]][j * pw+max_index[<span class="number">1</span>]]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> Out,Mcin</span><br></pre></td></tr></table></figure><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>调用之前自己写的手写数字识别神经网络框架（更名为<strong>neuralnet.py</strong>），该框架为3层全连接神经网络，神经元数量，学习率，正则化，损失函数等等均可自定义更改。详情见</p><p><a href="https://justzl.github.io/2023/10/31/%E6%89%8B%E5%86%99%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">手写神经网络手写数字识别—MINIST</a></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2012.png" alt="Untitled"></p><h2 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h2><p>训练过程如下</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2013.png" alt="Untitled"></p><p>构建网络后先进行一次前向传播以确立全连接网络形状，前向传播的过程省略，下面来看<strong>网络反向传播更新参数</strong>的过程。</p><h3 id="全连接层反向传播"><a href="#全连接层反向传播" class="headerlink" title="全连接层反向传播"></a>全连接层反向传播</h3><p>计算预测值和真实值的误差项，并逐层回传更新参数W和B，在误差传递到输入层时回传输出delta_back为池化层的误差，过程省略，可更改参数实现更多优化方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">delta_back,self.loss=self.network.backforward(self.nninput, </span><br><span class="line">     output_label[batch_size*batch:(batch+<span class="number">1</span>)*batch_size],</span><br><span class="line">     learning_rate=<span class="number">0.0052564</span>,</span><br><span class="line">     lossfunction=neuralnet.cross_entropy,</span><br><span class="line">     reg_strength=<span class="number">0</span>,</span><br><span class="line">     dfx=Leakyrelu_derivative,</span><br><span class="line">     adam=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="最大池化层反向传播"><a href="#最大池化层反向传播" class="headerlink" title="最大池化层反向传播"></a>最大池化层反向传播</h3><p>前文提到最大池化将每一窗口的最大值保留，其余丢弃。则<strong>池化层中输入数据的误差项为已保留数据的误差项</strong>，其余未保留数据的误差为0。利用前文存下的Mcin矩阵，则可以将误差项传给对应位置的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_pooling_derivate</span>(<span class="params">Mcin,delta,pool_size=(<span class="params"><span class="number">2</span>,<span class="number">2</span></span>)</span>):</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(Mcin.shape[<span class="number">0</span>]): <span class="comment">#对每一个样本</span></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(Mcin.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(delta.shape[<span class="number">2</span>]):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(delta.shape[<span class="number">3</span>]):</span><br><span class="line">                    Mcin[r][y][i*pool_size[<span class="number">0</span>]:(i+<span class="number">1</span>)*pool_size[<span class="number">0</span>]][j*pool_size[<span class="number">1</span>]:(j+<span class="number">1</span>)*pool_size[<span class="number">1</span>]]=Mcin[r][y][i*pool_size[<span class="number">0</span>]:(i+<span class="number">1</span>)*pool_size[<span class="number">0</span>]][j*pool_size[<span class="number">1</span>]:(j+<span class="number">1</span>)*pool_size[<span class="number">1</span>]]*delta[r][y][i][j]</span><br><span class="line">    <span class="keyword">return</span> Mcin</span><br></pre></td></tr></table></figure><h3 id="卷积层反向传播"><a href="#卷积层反向传播" class="headerlink" title="卷积层反向传播"></a>卷积层反向传播</h3><p>现在已知从池化层回传的误差，在将其经过激活函数的导数后回传到卷积层。卷积核上点A显然对卷积结果每一个点都有影响。它对卷积结果的影响等于将整个原图左上3×3的部分乘上点A的值，因此delta误差反向传播回时，<strong>点A的导数等于卷积结果的delta误差与原图左上3×3红色部分逐点相乘后求和</strong>。因此二维卷积核的导数等于原图对应通道与卷积结果对应通道的delta误差直接进行卷积<strong>。</strong></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2014.png" alt="Untitled"></p><p>我们的B是一个列向量，它给卷积结果的每一个通道都加上同一个标量。因此，在反向传播时，它的导数等于卷积结果的delta误差在每一个通道上将所有delta误差进行求和的结果。</p><script type="math/tex; mode=display">\frac{\partial C}{\partial w^l}  = \frac{\partial C}{\partial z^l}\frac{\partial z^l}{\partial w^l}=\delta^l*a^{l-1}\\\frac{\partial C}{\partial b^l}  = \frac{\partial C}{\partial z^l}\frac{\partial z^l}{\partial b^l}=\sum _{x}\sum _{y}\delta^l</script><p>这时我们之前存下的K矩阵就派上用场了，想求出卷积核的导数，只需将K矩阵和误差项直接矩乘即可得到卷积核的导数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">W_derivate</span>(<span class="params">W,delta,K</span>):</span><br><span class="line">    Y=np.zeros((W.shape[<span class="number">0</span>],W.shape[<span class="number">1</span>],W.shape[<span class="number">2</span>]*W.shape[<span class="number">3</span>],<span class="number">1</span>))</span><br><span class="line">    delta=delta.reshape((delta.shape[<span class="number">0</span>],delta.shape[<span class="number">1</span>],delta.shape[<span class="number">2</span>]*delta.shape[<span class="number">3</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span>(delta.shape[<span class="number">0</span>]):</span><br><span class="line">                Y[i,j]=Y[i,j]+np.dot(delta[rank,j].T,K[rank,i]).T</span><br><span class="line">    Yout=Y/delta.shape[<span class="number">0</span>]</span><br><span class="line">    Yout=Yout.reshape((W.shape[<span class="number">0</span>],W.shape[<span class="number">1</span>],W.shape[<span class="number">2</span>],W.shape[<span class="number">3</span>]))</span><br><span class="line">    <span class="keyword">return</span> Yout</span><br></pre></td></tr></table></figure><h3 id="adam优化算法"><a href="#adam优化算法" class="headerlink" title="adam优化算法"></a>adam优化算法</h3><p>卷积部分反向传播如下，对其使用adam优化算法，原理同全连接神经网络，这里不再赘述。</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2015.png" alt="Untitled"></p><h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><div class="table-container"><table><thead><tr><th>EPOCH</th><th>卷积层学习率</th><th>全连接学习率</th><th>优化算法</th><th>最低loss值</th><th>Test-ACC</th></tr></thead><tbody><tr><td>100</td><td>0.0085678</td><td>0.0052564</td><td>无</td><td>1.4319</td><td>47.74%</td></tr><tr><td>20</td><td>0.0085678</td><td>0.0052564</td><td>Adam</td><td>1.0998</td><td>63.6%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2016.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2017.png" alt="Untitled"></p><h1 id="基于pytorch搭建卷积神经网络"><a href="#基于pytorch搭建卷积神经网络" class="headerlink" title="基于pytorch搭建卷积神经网络"></a>基于pytorch搭建卷积神经网络</h1><h2 id="数据增强并导入"><a href="#数据增强并导入" class="headerlink" title="数据增强并导入"></a>数据增强并导入</h2><p><strong>调用torchvision的transforms库增强数据，利用datasets导入cifar数据集，使用dataloader进行数据分批。</strong></p><ul><li><p>数据增强</p><p>  数据增强可以帮助改善模型的泛化性能，减少过拟合。以下是使用的一些数据增强方法以及代码</p><p>  | 方法 | 作用 |<br>  | —- | —- |<br>  | RandomHorizontalFlip() | 进行随机水平翻转（镜像）以增加数据的多样性，帮助神经网络更好地学习不同角度的物体。 |<br>  | RandomGrayscale() | 以一定的概率随机将图像转换为灰度图像，以使模型对灰度图像的变化具有鲁棒性。 |<br>  | ColorJitter<br>  (brightness=0.2,contrast=0.2,saturation=0.2, hue=0.1) | 随机颜色变换 |<br>  | ToTenso() | 将图像数据转换为PyTorch张量（tensor），以便神经网络可以处理。 |<br>  | Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) | 对图像进行标准化，将像素值从范围[0, 1] 缩放到范围[-1, 1] |</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">transform_train = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(), </span><br><span class="line">    transforms.RandomGrayscale(),   </span><br><span class="line">transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0.1</span>),      </span><br><span class="line">transforms.ToTensor(),         </span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])  </span><br><span class="line">transform_test = transforms.Compose([     </span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br></pre></td></tr></table></figure></li><li><p>导入数据<br>使用<strong>DataLoader</strong>进行数据分批，<strong>dataset</strong>代表传入的数据集，<strong>batch_size</strong>表示每个batch有多少个样本，<strong>shuffle</strong>表示在每个epoch开始的时候，对数据进行<strong>重新排序。</strong></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">125</span></span><br><span class="line"><span class="comment">#将数据加载进来，本地已经下载好，    </span></span><br><span class="line">train_data = datasets.CIFAR10(root=<span class="string">r&#x27;.\data\cifar-10-python&#x27;</span>,train=<span class="literal">True</span>,transform=transform_train,download=<span class="literal">False</span>)</span><br><span class="line">test_data =datasets.CIFAR10(root=<span class="string">r&#x27;.\data\cifar-10-python&#x27;</span>,train=<span class="literal">False</span>,transform=transform_test,download=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#数据分批</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">train_loader = DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li>数据分批之前：<br><strong>torch.Size</strong>([3, 32, 32])：<br><strong>Tensor</strong>[[32<em>32][32</em>32][32*32]],<br>每一个元素都是归一化之后的RGB的值；</li><li><p>数据分批之前：<br><strong>train_data</strong>([50000[3<em>[32</em>32]]])</p></li><li><p>数据分批之后：<br><strong>torch.Size</strong>([64, 3, 32, 32])</p></li><li>数据分批之后：<br><strong>train_loader</strong>([50000/64<em>[64</em>[3<em>[32</em>32]]]])</li></ul><h2 id="设计网络结构"><a href="#设计网络结构" class="headerlink" title="设计网络结构"></a>设计网络结构</h2><h3 id="简单卷积神经网络"><a href="#简单卷积神经网络" class="headerlink" title="简单卷积神经网络"></a>简单卷积神经网络</h3><p>考虑较简单的卷积神经网络，即只有一层卷积层和一层池化层，以下是ConvNet1的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搭建卷积神经网络模型</span></span><br><span class="line"><span class="comment"># 1个卷积层,3个全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvNet1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvNet1, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">16</span>,kernel_size=<span class="number">2</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.1</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">            )</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">16</span> * <span class="number">16</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">128</span>, num_classes)</span><br><span class="line">        <span class="comment"># 定义前向传播顺序</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        out = self.fc3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_class_name</span>(<span class="params">cls</span>):</span><br><span class="line">        <span class="keyword">return</span> cls.__name__</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>网络结构</th><th>EPOCH</th><th>BN</th><th>优化算法</th><th>Test-ACC</th></tr></thead><tbody><tr><td>ConNet1</td><td>30</td><td>N</td><td>Adam</td><td>63.39%</td></tr><tr><td>ConNet1</td><td>30</td><td>Y</td><td>Adam</td><td>63.14%</td></tr><tr><td>ConNet1</td><td>30</td><td>N</td><td>SGD</td><td>63.33%</td></tr><tr><td>ConNet1</td><td>30</td><td>Y</td><td>SGD</td><td>64.78%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2018.png" alt="Untitled"></p><p>左图为分别使用Adam训练的有无添加BN的训练集和测试集loss，右图为是否启用BN的Acc</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2019.png" alt="Untitled"></p><p>同ConvNet1我们可以实现一个三层卷积层的神经网络ConvNet3，其训练结果如下</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2020.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2021.png" alt="Untitled"></p><p>为其添加dropout正则化，并在全连接层之间使用激活函数激活</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2022.png" alt="Untitled"></p><div class="table-container"><table><thead><tr><th>网络结构</th><th>EPOCH</th><th>dropout</th><th>优化算法</th><th>Test-ACC</th></tr></thead><tbody><tr><td>ConNet3</td><td>30</td><td>0.5</td><td>SGD</td><td>64.47%</td></tr><tr><td>ConNet3</td><td>30</td><td>0.1</td><td>SGD</td><td>72.29%</td></tr><tr><td>ConNet3</td><td>30</td><td>0.03</td><td>SGD</td><td>74.13%</td></tr><tr><td>ConNet3</td><td>30</td><td>0.001</td><td>SGD</td><td>70.55%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2023.png" alt="Untitled"></p><p>更多数据详情启动TensorBoard查看</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2024.png" alt="Untitled"></p><h3 id="较深卷积神经网络"><a href="#较深卷积神经网络" class="headerlink" title="较深卷积神经网络"></a>较深卷积神经网络</h3><p>可以看出，前面的两个网络都不能很好的学习到数据里的内容，不能很好的学会数据中的特征。我们加大网络的卷积层数，并加宽数据通道数，构建一个6层卷积神经网络ConvNet6（代码略）</p><p>我们采用5折交叉验证的方式进行训练，每个折训练30次，下图是模型在训练集和验证集上的loss曲线.数据详情见<strong>Tensorboard</strong></p><div class="table-container"><table><thead><tr><th>网络结构</th><th>EPOCH</th><th>dropout</th><th>优化算法</th><th>Test-ACC</th></tr></thead><tbody><tr><td>ConNet6</td><td>30</td><td>0</td><td>Adam</td><td>80.55%</td></tr><tr><td>ConNet6</td><td>30</td><td>0.35</td><td>Adam</td><td>81.3%</td></tr><tr><td>ConNet6</td><td>30</td><td>0.5</td><td>Adam</td><td>81.86%</td></tr><tr><td>ConNet6</td><td>30</td><td>0.7</td><td>Adam</td><td>81.14%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2025.png" alt="Untitled"></p><p>还可以看到每一次卷积后图像的变化，以Convnet6为例</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2026.png" alt="Untitled"></p><h1 id="基于预训练模型搭建卷积神经网络"><a href="#基于预训练模型搭建卷积神经网络" class="headerlink" title="基于预训练模型搭建卷积神经网络"></a>基于预训练模型搭建卷积神经网络</h1><h2 id="测试不同模型"><a href="#测试不同模型" class="headerlink" title="测试不同模型"></a>测试不同模型</h2><div class="table-container"><table><thead><tr><th>模型</th><th>优化算法</th><th>准确率</th></tr></thead><tbody><tr><td>VGG16</td><td>Adam</td><td>85.34%</td></tr><tr><td>ResNet50</td><td>Adam</td><td>83.2%</td></tr><tr><td>DenseNet121</td><td>Adam</td><td>86.02%</td></tr></tbody></table></div><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2027.png" alt="Untitled"></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2028.png" alt="Untitled"></p><p><strong>利用tensorboard的addgraph功能可以清晰的看到各个模型内部的结构以及数据张量的流动</strong></p><p><strong>（高清图见tensorboard内ConvolutionNN\logs\model_vision）</strong></p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2029.png" alt="Untitled"></p><h2 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h2><p>在预测时，对所有模型的输出，采用投票的形式决定最终的预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnsembleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, models</span>):</span><br><span class="line">        <span class="built_in">super</span>(EnsembleModel, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> models <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.models = nn.ModuleList(models)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.models = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">if</span> self.models <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> inputs  <span class="comment"># Pass through the input if there are no models in the ensemble</span></span><br><span class="line">        outputs = [model(inputs) <span class="keyword">for</span> model <span class="keyword">in</span> self.models]</span><br><span class="line">        <span class="keyword">return</span> torch.stack(outputs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入模型</span></span><br><span class="line">model_vgg16 = torch.load((<span class="string">r&#x27;.\CNNmodel\VGG16Adambestfold.pt&#x27;</span>))</span><br><span class="line">model_resnet50 = torch.load((<span class="string">r&#x27;.\CNNmodel\ResNet50Adambestfold.pt&#x27;</span>))</span><br><span class="line">model_densenet101 = torch.load((<span class="string">r&#x27;.\CNNmodel\DenseNet121Adambestfold.pt&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型集成</span></span><br><span class="line">models = [model_vgg16, model_resnet50, model_densenet101]</span><br><span class="line">ensemble_model = EnsembleModel(models)</span><br><span class="line">predictions = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images = images.to(device)  </span><br><span class="line">        outputs = ensemble_model(images)</span><br><span class="line">        <span class="comment"># 统计每个模型的预测结果# 将输入数据移动到GPU</span></span><br><span class="line">        ensemble_predictions = outputs.<span class="built_in">max</span>(dim=<span class="number">2</span>)[<span class="number">0</span>].mode(dim=<span class="number">0</span>).values</span><br><span class="line">        predictions.extend(ensemble_predictions.cpu().numpy())</span><br><span class="line">         <span class="comment"># 计算准确率</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predictions == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy on the test set: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算最终的投票结果</span></span><br><span class="line">final_predictions = [torch.mode(torch.tensor(prediction))[<span class="number">0</span>].item() <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions]</span><br></pre></td></tr></table></figure><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2030.png" alt="Untitled"></p><h1 id="附录：TensorBoard可视化使用（vscode）"><a href="#附录：TensorBoard可视化使用（vscode）" class="headerlink" title="附录：TensorBoard可视化使用（vscode）"></a>附录：TensorBoard可视化使用（vscode）</h1><p>tensorboard是一个非常强大的工具、不仅仅可以帮助我们可视化神经网络训练过程中的各种参数，而且可以帮助我们更好的调整网络模型、网络参数。此处仅介绍在代码中的基本使用。（先在vscode扩展中下载tensroboard。）</p><p>在代码中启动tensorboard，logdir为你想存放日志文件的位置，即生成数据的位置</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2031.png" alt="Untitled"></p><p>使用addscaler函数添加loss值，第一个参数为图名称，第二个为loos值，第三个为迭代次数</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2032.png" alt="Untitled"></p><p>使用addtext函数添加text值，这里用于存放每一次预测准确率</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2033.png" alt="Untitled"></p><p>使用addgraph函数添加模型计算图，testinput为样例输入</p><p><img src="/2023/10/31/%E6%89%8B%E5%86%99%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Untitled%2034.png" alt="Untitled"></p><p>Tensorboard类的参数列表如下</p><ul><li><p><strong>log_dir:</strong></p></li><li><p>用来保存被 TensorBoard 分析的日志文件的文件名。</p></li><li><p><strong>histogram_freq</strong></p></li><li><p>对于模型中各个层计算激活值和模型权重直方图的频率（训练轮数中）。 如果设置成 0 ，直方图不会被计算。对于直方图可视化的验证数据（或分离数据）一定要明确的指出。</p></li><li><p><strong>write_graph</strong>:</p></li><li><p>是否在 TensorBoard 中可视化图像。 如果 write_graph 被设置为 True。</p></li><li><p><strong>write_grads</strong>:</p></li><li><p>是否在 TensorBoard 中可视化梯度值直方图。 histogram_freq 必须要大于 0 。</p></li><li><p><strong>batch_size</strong>:</p></li><li><p>用以直方图计算的传入神经元网络输入批的大小。</p></li><li><p><strong>write_images</strong>:</p></li><li><p>是否在 TensorBoard 中将模型权重以图片可视化，如果设置为True，日志文件会变得非常大。</p></li><li><p><strong>embeddings_freq</strong>:</p></li><li><p>被选中的嵌入层会被保存的频率（在训练轮中）。</p></li><li><p><strong>embeddings_layer_names</strong>:</p></li><li><p>一个列表，会被监测层的名字。 如果是 None 或空列表，那么所有的嵌入层都会被监测。</p></li><li><p><strong>embeddings_metadata</strong>:</p></li><li><p>一个字典，对应层的名字到保存有这个嵌入层元数据文件的名字。 查看 详情 关于元数据的数据格式。 以防同样的元数据被用于所用的嵌入层，字符串可以被传入。</p></li><li><p><strong>embeddings_data</strong>:</p></li><li><p>要嵌入在 embeddings_layer_names 指定的层的数据。 Numpy 数组（如果模型有单个输入）或 Numpy 数组列表（如果模型有多个输入）。 Learn ore about embeddings。</p></li><li><p><strong>update_freq</strong>:</p></li><li><p>‘batch’ 或 ‘epoch’ 或 整数。当使用 ‘batch’ 时，在每个 batch 之后将损失和评估值写入到 TensorBoard 中。同样的情况应用到 ‘epoch’ 中。如果使用整数，例如 10000，这个回调会在每 10000 个样本之后将损失和评估值写入到 TensorBoard 中。注意，频繁地写入到 TensorBoard 会减缓你的训练。</p></li></ul><p><a href="https://www.notion.so/pytorch-7eb2d2014d4d4e7f8b291ffe7327fb31?pvs=21">手写卷积神经网络与pytorch神经网络说明</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>HMM</title>
      <link href="/2023/05/17/HMM/"/>
      <url>/2023/05/17/HMM/</url>
      
        <content type="html"><![CDATA[<h1 id="HMM学习"><a href="#HMM学习" class="headerlink" title="HMM学习"></a>HMM学习</h1><p>HMM是隐马尔可夫模型（Hidden Markov Model）的缩写，是一种用于表示时间序列数据的概率模型，常用于语音识别、自然语言处理、生物信息学等领域。</p><p>使用HMM模型时我们的问题一般有这两个特征：</p><ol><li>我们的问题是<strong>基于序列</strong>的，比如时间序列，或者状态序列。</li><li>我们的问题中有两类数据，<strong>一类序列数据是可以观测到的</strong>，即观测序列；而<strong>另一类数据是不能观察</strong>到的，即隐藏状态序列，简称状态序列。</li></ol><h1 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h1><p>假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里球的数量分别是：</p><div class="table-container"><table><thead><tr><th>盒子</th><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>红球数</td><td>5</td><td>4</td><td>7</td></tr><tr><td>白球数</td><td>5</td><td>6</td><td>3</td></tr></tbody></table></div><p>再开始抽球之前，对每个盒子都存在初始概率，即第一次抽到1/2/3号盒子的概率，假设是0.2/0.4/0.4,此为<strong>初始状态矩阵</strong></p><script type="math/tex; mode=display">\pi=(0.2,0.4,0.4)^{T}</script><p>抽球的规则必须遵守如下：</p><ul><li>我们无法看到球从哪个盒子里被取出，我们<strong>只能看到取出球的颜色。</strong>假设取出的是红球，其从1号盒子出来的概率是0.5，从2号盒子里出来的概率是0.4，从3号盒子里出来的概率是0.7，我们就得到了红球的观察状态概率矩阵，同理假设白球从1号盒子里取出的概率是0.5 ，从2号盒子里出来的概率是0.6，从3号盒子里出来的概率是0.3，就得到了<strong>发射概率矩阵</strong></li></ul><pre><code>|  | 红球 | 白球 || --- | --- | --- || 1号盒子 | 0.5 | 0.5 || 2号盒子 | 0.4 | 0.6 || 3号盒子 | 0.7 | 0.3 |</code></pre><script type="math/tex; mode=display">B=\left(\begin{array}{ll}0.5 & 0.5 \\0.4 & 0.6 \\0.7 & 0.3\end{array}\right)</script><ul><li>在每一个盒子抽完一次球（并放回）后，就会有概率转移到别的盒子（或自己）继续进行抽取。假设某一次是从1号盒子抽取的，下一次从一号盒子抽取的概率是0.5，下一次从二号盒子抽取的概率是0.2，下一次从3号盒子抽取的概率是0.3。我们就得到了从1号盒子出发的状态转移概率（0.5  0.2  0.3），同理假设从2号3号转移到下一个盒子，就得到了<strong>状态转移矩阵</strong></li></ul><pre><code>|  | 1号盒子 | 2号盒子 | 3号盒子 || --- | --- | --- | --- || 1号盒子 | 0.5 | 0.2 | 0.3 || 2号盒子 | 0.3 | 0.5 | 0.2 || 3号盒子 | 0.2 | 0.3 | 0.5 |</code></pre><script type="math/tex; mode=display">A=\left(\begin{array}{lll}0.5 & 0.2 & 0.3 \\0.3 & 0.5 & 0.2 \\0.2 & 0.3 & 0.5\end{array}\right)</script><h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>隐马尔可夫模型由<strong>状态序列</strong>、<strong>观测序列</strong>和<strong>两个概率矩阵</strong>组成。状态序列是隐含的、不可见的，而观测序列是可观测的。隐马尔可夫模型的基本假设是，观测序列取决于对应的状态序列，在给定状态序列的情况下，观测序列的概率分布是已知的，并且状态之间的转移满足马尔可夫性质。</p><p>首先我们假设<strong>Q是所有可能的隐藏状态的集合</strong>，<strong>V是所有可能的观测状态的集合</strong></p><script type="math/tex; mode=display">Q=\left\{q_{1}, q_{2}, \ldots, q_{N}\right\}, V=\left\{v_{1}, v_{2}, \ldots v_{M}\right\}</script><p>N是可能的隐藏状态数，M是所有的可能的观察状态数。对于长度为T的序列，I是对应的<strong>隐藏状态序列</strong>，Q是对应的<strong>观测序列</strong></p><script type="math/tex; mode=display">I=\left\{i_{1}, i_{2}, \ldots, i_{T}\right\}, O=\left\{o_{1}, o_{2}, \ldots o_{T}\right\}，i_{t} \in Q，o_{t}\in V</script><h2 id="两个基本假设"><a href="#两个基本假设" class="headerlink" title="两个基本假设"></a>两个基本假设</h2><ol><li><p>齐次马尔可夫性假设（一阶马尔可夫性假设）</p><p> 假设隐藏的马尔科夫链在任意时刻的状态<strong>只依赖于前一时刻的状态</strong>，与其他时刻的状态及观测时间 $t$ 无关，如果在时刻 $t$ 的隐藏状态是   $i_{t}=q_{i}$ ，在时刻  $t+1$ 的隐藏状态是 $i_{ t+1 }=q_{j}$ ，则从时刻 $t$ 到时刻  $t+1$ 的HMM状态转移概率 $a_{i j}$ 可以表示为:</p><script type="math/tex; mode=display"> a_{i j}=P\left(i_{t+1}=q_{j} \mid i_{t}=q_{i}\right)</script><p> 这样 $a_{i j}$ 可以组成<strong>状态转移矩阵</strong> A</p><script type="math/tex; mode=display"> A=[a_{i j}]_{N * N}</script></li><li><p>观测独立性假设</p><p> 假设任意时刻的<strong>观察状态</strong>只<strong>仅仅</strong>依赖于<strong>当前时刻的隐藏状态</strong>，这也是一个为了简化模型的假设，如果在时刻 $t$ 的隐藏状态是   $i_{t}=q_{i}$ ，对应的观察状态为 $o_{t}=v_{k}$ ,则该时刻观测状态$v_{k}$在隐藏状态$q_{j}$下生成的概率 $b_j(k)$ 为</p><script type="math/tex; mode=display"> b_{j}(k)=P(o_{t}=v_{k}|i_{t}=q_{t})</script><p> 这样 $b_j(k)$ 可以组成<strong>观测概率矩阵</strong> B</p><script type="math/tex; mode=display"> B=[b_{j}(k)]_{N * M}</script><p> 除此以外，我们需要一组在初始时刻下的隐藏状态概率分布 $\pi$</p><script type="math/tex; mode=display"> \pi =[\pi(i)]_{N} , \pi(i)=P(i_{1}=q_{i})</script><p> 因此，一个HMM模型由三元组 $\lambda$ 构成 , $\lambda =(A ,B,\pi)$，观察测序列生成过程如下图</p><p> <img src="/2023/05/17/HMM/Untitleds.png" alt="Untitled"></p><h1 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h1><h2 id="概率计算问题（评估问题）"><a href="#概率计算问题（评估问题）" class="headerlink" title="概率计算问题（评估问题）"></a>概率计算问题（评估问题）</h2><p> 给定模型$\lambda =(A,B,\pi)$ 和观测序列 $O$,求在该模型下观测序列出现的概率 $P(O|\lambda)$</p><p> 本质可以理解为：在这个$\lambda$模型下，在所有生成的观测序列中求题给的观测序列出现的概率</p><p> 因此可以直接通过<strong>暴力计算算出。</strong></p><h3 id="暴力计算"><a href="#暴力计算" class="headerlink" title="暴力计算"></a>暴力计算</h3><p> 核心：通过枚举遍历所有的隐藏状态序列，再求出<strong>在所有隐藏状态序列</strong>中<strong>生成的观测序列</strong>为<strong>O</strong>的序列概率，即求:$P(O,I|\lambda)$</p><script type="math/tex; mode=display"> I=(i_{1},i_{2},...,i_{T}),i\in Q</script><p> 其中Q有N种状态，i一共有N种取值，则序列I一共有 $T\times N^{T}$ 种可能</p><p> 思路：先<strong>求隐藏状态序列</strong>的生成概率 $P(I|\lambda)$ ，再求<strong>从状态序列生成观测序列的概率</strong>（就是乘以矩阵B） $P(O|I,\lambda)$，通过联合概率公式得</p><script type="math/tex; mode=display"> P(O,I|\lambda)=P(O|I,\lambda)P(I|\lambda)</script><p> 这就求出了隐藏状态序列 $I=(i_{1},i_{2},…,i_{T})$ 生成观测序列 $O$ 的概率，把所有隐藏状态序列都这样求和即得该模型能生成此观测状态序列 $O$ 的概率</p><script type="math/tex; mode=display"> P(O|\lambda)=\sum_{I}^{} P(O,I|\lambda)=\sum_{I}^{} P(O|I,\lambda)P(I|\lambda)</script><h3 id="前向算法：（从初始状态，递推求要怎么走才能走到想要的结果O）"><a href="#前向算法：（从初始状态，递推求要怎么走才能走到想要的结果O）" class="headerlink" title="前向算法：（从初始状态，递推求要怎么走才能走到想要的结果O）"></a>前向算法：（从初始状态，递推求要怎么走才能走到想要的结果O）</h3><p> 核心：给定模型 $\lambda =(A,B,\pi)$ ，定义到时刻$t$部分观测序列 $o_{1},o_{2},….,o_{t}$, 且状态为$q_{i}$的概率为<strong>前向概率</strong></p><script type="math/tex; mode=display"> \alpha_{t}(i) = P(o_{1},o_{2},... ,o_{t} ,i_{t}=q_{i}|\lambda)</script><ol><li><p>计算初值（初始状态）</p><p> $\pi_{i}$ 表示初始选择的隐藏状态的概率, $b_{i}(o_{1})$ 表示每一种隐藏状态发射生成观测状态$o_{1}$的概率,$N$为隐藏状态的个数</p><script type="math/tex; mode=display"> \alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), i=1,2, \cdots, N</script></li><li><p>递推直至序列末</p><p> 对 $t=1,2,….,T-1$ :</p><script type="math/tex; mode=display"> \alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), i=1,2, \cdots, N</script><p> 其中，$\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right]$ 表示 $t+1$ 时刻第 $i$ 个隐藏状态的出现概率，由<strong>齐次马尔可夫假设</strong>，此概率由前一时刻的状态通过状态转移概率（矩阵A）而来，而前一时刻每种状态都有概率转换到此状态，<strong>故$t+1$时刻状态出现的概率为$t$时刻所有状态经状态转移之后的和，下图更为直观</strong></p><p> <img src="/2023/05/17/HMM/Untitled.png" alt="t时刻前向算法的递推过程"></p><p> t时刻前向算法的递推过程</p><p> $b_{i}(o_{t+1})$ 表示每一种隐藏状态发射生成观测状态 $o_{t+1}$ 的概率</p></li><li><p>终止递推并输出</p><p> 在序列最终时刻T，由 $\alpha_{T}(i)=P(o_{1},o_{2},…,o_{T},i_{T}=q_{i}|\lambda)$ ，所有隐藏状态出现的概率之和即为观测序列出现的概率。</p><script type="math/tex; mode=display"> P（O|\lambda)=\sum_{i=1}^{N} \alpha_{T}(i)</script></li></ol></li></ol><pre><code>### 后向算法：（知道结果O，一直递推前一步求当初是怎么走过来的）核心：给定模型 $\lambda =(A,B,\pi)$ ，定义到时刻 $t$ 且状态为 $q_&#123;i&#125;$ 的条件下，从 $t+1$ 到 $T$ 的部分观测序列 $o_&#123;t+1&#125;,o_&#123;t+2&#125;,...,o_&#123;T&#125;$ 概率为**后向概率**$$\beta_&#123;t&#125;(i)=P(o_&#123;t+1&#125;,o_&#123;t+2&#125;,...,o_&#123;T&#125;|i_&#123;t&#125;=q_&#123;i&#125;,\lambda)$$4. 初始化序列末尾t=T    $$    \beta_&#123;T&#125;(i)=1,i=1,2,....,N    $$5. 递推直至序列开始    $对t=T-1,T-2,...,1$    $$    \beta_&#123;t&#125;(i)=\sum_&#123;j=1&#125;^&#123;N&#125;a_&#123;ij&#125;b_&#123;j&#125;(o_&#123;t+1&#125;)\beta_&#123;t+1&#125;(j) ,i=1,2,3....,N     $$6. 计算求和    $$    P(O|\lambda)=\sum_&#123;i=1&#125;^&#123;N&#125;\pi_&#123;i&#125;b_&#123;i&#125;(o_&#123;1&#125;)\beta_&#123;1&#125;(i)     $$利用前向概率和后向概率的定义可以将观测序列概率统一写为$$P(O \mid \lambda)=\sum_&#123;i=1&#125;^&#123;N&#125; \sum_&#123;j=1&#125;^&#123;N&#125; \alpha_&#123;t&#125;(i) \alpha_&#123;i j&#125; b_&#123;j&#125;\left(o_&#123;t+1&#125;\right) \beta_&#123;t+1&#125;(j), t=1,2, \cdots, T-1$$### **利用前向后向概率，还可以得到关于单个状态和两个状态概率的计算公式**- 给定模型 $\lambda$ 和观测 $O$ ，在时刻 $t$ 处于状态 $q_i$ 的概率    $$    \gamma_t(i)=P(i_t=q_i|O,\lambda)    $$    则由前向后向概率定义    $$    \alpha_&#123;t&#125;(i)*\beta_&#123;t&#125;(i)=P(i_t=q_i,O|\lambda)    $$    $$    \gamma_t(i)=P(i_t=q_i|O,\lambda)=\frac&#123;P(i_&#123;t&#125;=q_&#123;i&#125;,O|\lambda)&#125;&#123;P(O|\lambda)&#125;\\    =\frac&#123;\alpha_&#123;t&#125;(i)*\beta_&#123;t&#125;(i)&#125;&#123;P(O|\lambda)&#125;=\frac&#123;\alpha_&#123;t&#125;(i)*\beta_&#123;t&#125;(i)&#125;&#123;\sum_&#123;j=1&#125;^&#123;N&#125;\alpha_&#123;t&#125;(j)*\beta_&#123;t&#125;(j)&#125;    $$- 给定模型$\lambda$和观测$O$，在时刻$t$处于状态$q_i$的概率,且在时刻$t$处于状态$q_j$的概率$$\xi_t(i,j)=P(i_t=q_i,i_&#123;t+1&#125;=q_j|O,\lambda)=\frac&#123;P(i_t=q_i,i_&#123;t+1&#125;=q_j,O|\lambda)&#125;&#123;P(O|\lambda)&#125;\\=\frac&#123;\alpha_t(i)a_&#123;ij&#125;b_j(o_&#123;t+1&#125;)\beta_&#123;t+1&#125;(j)&#125;&#123;\sum_&#123;i=1&#125;^&#123;N&#125;\sum_&#123;j=1&#125;^&#123;N&#125;\alpha_t(i)a_&#123;ij&#125;b_j(o_&#123;t+1&#125;)\beta_&#123;t+1&#125;(j)&#125;$$## 学习问题## 预测（解码）问题### 近似算法核心：在每个时刻选择该时刻最有可能出现的下一个状态，从而得到一个状态序列。![viterbi.png](HMM/viterbi.png)### 维特比算法Viterbi algorithm维特比算法是依据动态规划原理求概率最大路径的，其有一个特性：**如果在t时刻通过一个节点$t^&#123;*&#125;$，对于从$t_1$ 到$t^&#123;*&#125;$的所有路径中，这条路径必须是最优的。**下面来看看这个算法直观理解假设我们的序列$T=3$。首先从起点开始选择第一个隐藏状态A，这时我们虽然可以通过$\pi$矩阵知道各个状态转移概率，但我们无法保证选择的某一个状态（A1或A2或A3）一定是在整条最优状态序列中的。![Untitled](HMM/Untitled%201.png)所以我们继续看B状态，依据动态规划原理，如果将来的最优路径要经过B1，那么，**这条路径一定包括从起点到B1的最短路径**，否则就会有更优解替代这条最优路径。在已知概率转移矩阵的情况下，我们很容易计算并比对出一条**最优路径**，假设是$S-A_1-B_1$ 。那么其余的经过B1的路径$S-A_2-B_1 和 S-A_3-B_1$ 就会**被舍弃（这是viterbi算法相比暴力遍历所有路径节省时间复杂度的关键）**![Untitled](HMM/Untitled%202.png)同理可计算出，将来要经过B2和B3的最优路径,假设是$S-A_3-B_2 和 S-A_2-B_3$ ，这样我们就得到了从S到B1B2B3的三条最优路径。![Untitled](HMM/Untitled%203.png)接下来看经过C状态，首先看经过C1状态，如果用**暴力遍历**的话，这一轮我们至少要遍历**9*3=18**条路径才能找得到最优路径，但由于我们在前一轮已经舍弃了很多条路径，所以这一轮我们只需要从$S-A_1-B_1-C_1 和 S-A_2-B_3-C_1和 S-A_3-B_2-C_1$ 这三条路径中选择最优的路径，这大大简化了计算规模，**最终假设我们选择了$S-A_3-B_2-C_1$  这条路径为经过C1的最优路径**。同理可得经过C2和C3的最优路径![Untitled](HMM/Untitled%204.png)那么当动态规划执行到状态序列最后时，我们只需从三条序列中选择最优的一条，这一条就是我们要找的全局最优序列![Untitled](HMM/Untitled%205.png)### 直观对比：近似算法的核心是从已经选择的节点寻找最优可能的下一个节点，而viterbi算法的核心是找到从**起点**到**欲通过节点**前的最优可能路径，在遇到每下一个欲通过状态时都会考虑N个路径。![Untitled](HMM/Untitled%206.png)### 具体算法：为了实现viterbi算法，我们需要定义两个变量定义变量$\delta$ ,它的作用是求在时刻$t$状态为$q_i$的所有路径中概率最大的**值**，例如$\delta_1(2)$ 就是求$t_1$时刻所有经过状态2的路径中概率最大的值（如上图的$S-A_3-B_2$ 的值）$$\delta _t(i)=\max_&#123;i_1,i_2,..,i_&#123;t-1&#125;&#125;P(i_t=i,i_&#123;t-1&#125;,...,i_1,o_t,...,o_1|\lambda),i=1,2,....,N$$利用前向概率可以得到它的递推公式$$\begin&#123;aligned&#125;\delta_&#123;t+1&#125;(i) &amp; =\max _&#123;i_&#123;1&#125;, i_&#123;2&#125;, \cdots, i_&#123;t&#125;&#125; P\left(i_&#123;t+1&#125;=i, i_&#123;t&#125;, \cdots, i_&#123;1&#125;, o_&#123;t+1&#125;, \cdots, o_&#123;1&#125; \mid \lambda\right) \\&amp; =\max _&#123;1 \leqslant j \leqslant N&#125;\left[\delta_&#123;t&#125;(j) a_&#123;j i&#125;\right] b_&#123;i&#125;\left(o_&#123;t+1&#125;\right), \quad i=1,2, \cdots, N ; \quad t=1,2, \cdots, T-1\end&#123;aligned&#125;$$有了这个变量，我们就可以递推到最后找到概率最大的节点，但是光有值还不够，我们还需要完整的一条状态序列。这时就需要viterbi算法来**从后向前回溯寻找序列节点**，定义变量$\Psi$ $$\psi_t(i)=\arg\max_&#123;1\le j\le N&#125;\left[\delta_&#123;t-1&#125;(j)a_&#123;ji&#125;\right],i=1,2,...,N$$1. 初始化    $$    \delta_1(i)=\pi_ib_i(o_1),i=1,2,3...,N\\    \psi_1(i)=0,i=1,2,3...,N    $$2. 递推计算每个时刻t经过状态i的概率最大值（N表示隐藏状态数，即对每个隐藏状态都计算一条路径）    对$t=2,3...,T$    $$     \delta_t(i)=\max _&#123;1 \leqslant j \leqslant N&#125;\left[\delta_&#123;t-1&#125;(j) a_&#123;j i&#125;\right] b_&#123;i&#125;\left(o_&#123;t&#125;\right), \quad i=1,2, \cdots, N \\    \psi_t(i)=\arg\max_&#123;1\le j\le N&#125;\left[\delta_&#123;t-1&#125;(j)a_&#123;ji&#125;\right],i=1,2,...,N    $$3. 终止    $$    P^&#123;*&#125;=\max_&#123;1\le i\le N&#125;\delta_T(i)\\    i^*_T=\arg\max_&#123;1\le i\le N&#125;[\delta_T(i)]    $$4. 回溯最优路径节点    对$t=T-1,T-2,...,1$    $$    i_t^*=\psi_&#123;t+1&#125;(i_&#123;t+1&#125;^*)    $$    可得最优路径$I^*=(i_1^*,i_2^*,...,i_T^*)$# 例题计算[HMM习题-蒋子龙](https://www.notion.so/HMM-d381c07ee64f4783b7b037bed10f20b9)[3 中文分词 - 隐马尔科夫模型分词](https://www.notion.so/3-4576f825051f4d06b0d97ee5b08b99e1)</code></pre>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
